{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$P(A|B) = \\frac{P(AB)}{P(B)}$$\n",
    "朴素贝叶斯的特征：\n",
    "1. 假设每个变量相互独立不干扰，比如假设 bacon出现在delicious边上和bacon出现在unhealthy边上的可能相同\n",
    "2. 假设每个变量都同样重要\n",
    "\n",
    "显然，以上假设在实际中都是不正确的，但忽略这些细节，朴素贝叶斯在实际应用中却很有效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类言论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理步骤：\n",
    "1. 获取所有文章中所有词的集合\n",
    "2. 获取每一篇文章的向量(文章中词在集合中是否出现，出现为1，不出现为0)\n",
    "3. 利用以下公式\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(c_i|w) &= \\frac{P(w|c_i)P(c_i)}{P(w)} \\\\\n",
    "w&: 文章的向量  \\\\ \n",
    "c_i&: 文章的分类结果  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中：$$ P(c_i) = \\frac{c_i出现频数}{文章总数量} $$\n",
    "其中：$$ P(w|c_i) = P((w_0,w_1, \\cdots, w_n)|c_i) $$\n",
    "根据朴素贝叶斯独立性假设，以上公式变为：\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "P(w|c_i) &= P(w_0|c_i)P(w_1|c_i)\\cdots P(w_n|c_i) \\\\ \n",
    "其中：P(w_0|c_i) &= \\frac{c_i出现时候w_0出现的总频数和}{c_i类所有文章的词的数量和}\n",
    "\\end{aligned}\n",
    "$$\n",
    "对于一个输入w，其P(w)已经是确定值了，所以在这里可以不考虑分母，因为分母都一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList,classVec = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Navie_Bayes:\n",
    "    '''token_list:每一元素都是一个列表，该列表是一篇文章的分词结果\n",
    "       classVec  ：每一个元素都对应token_list每个列表(文章的)的分类结果，比如1表示垃圾文章\n",
    "    '''\n",
    "    def __init__(self, token_list, classVec):\n",
    "        self.token_list = token_list\n",
    "        self.classVec = classVec\n",
    "        self.vacabulary, self.vacabulary_index = self.__get_vacabulary()\n",
    "        self.p0v, self.p1v, self.pclass1 = self.__train_probility()\n",
    "        \n",
    "    #获取所有数据中单词的集合\n",
    "    def __get_vacabulary(self):\n",
    "        tokens = []\n",
    "        for words in self.token_list:\n",
    "            tokens += words \n",
    "        tokens = set(tokens)\n",
    "        index_dict = {}\n",
    "        for _a, word in enumerate(tokens):\n",
    "            index_dict[word] = _a\n",
    "        return tokens, index_dict\n",
    "    \n",
    "    #获取inputset的向量值，1表示该词出现\n",
    "    def setOfWord2Vec(self, inputset):\n",
    "        articel_vec = [0]*len(self.vacabulary)  #创建和vacabulary同维度的向量\n",
    "        for word in inputset:\n",
    "            if word in self.vacabulary:\n",
    "                articel_vec[self.vacabulary_index[word]] = 1\n",
    "            else:\n",
    "                pass\n",
    "                #print('the word {} is not in my vocabulary'.format(word))\n",
    "        return articel_vec\n",
    "    \n",
    "    #计算P(w0|ci)和P(ci)\n",
    "    #改进1：为了避免P(w0|ci)中分子为0，导致整个P(w|ci)为0的情况，初始化occurrence为1\n",
    "    #改进2：因为计算出来的概率小，所有概率的乘积会更小，这样有可能会丢失精度，在这里对求出的概率取自然数对数处理\n",
    "    def __train_probility(self):\n",
    "        #train_matircs文章向量列表，train_vec是文章分类结果\n",
    "        train_matrics = []\n",
    "        for words in self.token_list:\n",
    "            train_matrics.append(self.setOfWord2Vec(words))\n",
    "        train_vec = self.classVec\n",
    "        num_train_article = len(train_matrics)\n",
    "        num_word = len(train_matrics[0])\n",
    "        prob_c1 = sum(train_vec) / num_train_article\n",
    "        prob_w_c0 = np.ones(num_word)   #从zeros改为ones,初始化分子为1 改进1\n",
    "        prob_w_c1 = np.ones(num_word)   #从zeros改为ones，初始化分子为1 改进1\n",
    "        num_c0 = 2  #初始化分母从0改为2  改进1\n",
    "        num_c1 = 2  #初始化分母从0改为2  改进1\n",
    "        \n",
    "        for i in range(num_train_article):\n",
    "            if train_vec[i] == 1:\n",
    "                prob_w_c1 += train_matrics[i]\n",
    "                num_c1 += sum(train_matrics[i])\n",
    "            else:\n",
    "                prob_w_c0 += train_matrics[i]\n",
    "                num_c0 += sum(train_matrics[i])                \n",
    "        prob1_vec = np.log(prob_w_c1/num_c1)\n",
    "        prob0_vec = np.log(prob_w_c0/num_c0)\n",
    "        \n",
    "        return prob0_vec, prob1_vec , prob_c1\n",
    "    \n",
    "    def classifyNB(self, input_article):\n",
    "        #因为转换为log所以和即是概率的乘积\n",
    "        input_vec = self.setOfWord2Vec(input_article)\n",
    "        #矩阵*表示单个单个元素对应相乘，维度要一样，因为p1v保存了所有P(wi|ci)，\n",
    "        #要求input_vec的P(wi|ci)，只要把p1v中属于input_vec的项提取出来即可\n",
    "        p1 = sum(input_vec * self.p1v) + np.log(self.pclass1) \n",
    "        p2 = sum(input_vec * self.p0v) + np.log(1-self.pclass1)\n",
    "        if p1 > p2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_navie_byes = Navie_Bayes(postingList,classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_navie_byes.classifyNB(['love','my','dalmation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_navie_byes.classifyNB(['stupid','garbage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([[1,1,1,1],[2,2,2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words 词袋模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在1.2种获取一篇文章的向量时候，我们定义了一个setOfWord2Vec函数来实现。\n",
    "2. 但是1.2种获取的向量没有考虑到当文章中某次多次出现的情况(多次和一次都是一样效果)，向量只包含0,1。\n",
    "3. 为了解决上述缺陷，提出词袋模型，并对setOfWord2Vec做出修改,某词出现时其特征对应元素+1，重命名为bagOfWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "class Navie_Bayes_BOW:\n",
    "    '''token_list:每一元素都是一个列表，该列表是一篇文章的分词结果\n",
    "       classVec  ：每一个元素都对应token_list每个列表(文章的)的分类结果，比如1表示垃圾文章\n",
    "    '''\n",
    "    def __init__(self, token_list, classVec):\n",
    "        self.token_list = token_list\n",
    "        self.classVec = classVec\n",
    "        self.vacabulary, self.vacabulary_index = self.__get_vacabulary()\n",
    "        self.p0v, self.p1v, self.pclass1 = self.__train_probility()\n",
    "        \n",
    "    #获取所有数据中单词的集合\n",
    "    def __get_vacabulary(self):\n",
    "        tokens = []\n",
    "        for words in self.token_list:\n",
    "            tokens += words \n",
    "        #移除高频词，因为高频词大多数都是连接等关系，等于语意无关的词，如果停用次比较全可以不这样处理\n",
    "        #token_count = Counter(tokens)\n",
    "        #token_count_sorted = list(sorted(token_count.items() , key=lambda x:x[1], reverse=True))\n",
    "        #tokens = [token[1] for token in token_count_sorted[20:]]\n",
    "        tokens = set(tokens)\n",
    "        index_dict = {}\n",
    "        for _a, word in enumerate(tokens):\n",
    "            index_dict[word] = _a\n",
    "        return tokens, index_dict       \n",
    "    \n",
    "    #获取inputset的向量值，1表示该词出现\n",
    "    def bagOfWord2Vec(self, inputset):\n",
    "        articel_vec = [0]*len(self.vacabulary)  #创建和vacabulary同维度的向量\n",
    "        for word in inputset:\n",
    "            if word in self.vacabulary:\n",
    "                articel_vec[self.vacabulary_index[word]] += 1 # =1 变为 +=1 \n",
    "            else:\n",
    "                pass\n",
    "                #print('the word {} is not in my vocabulary'.format(word))\n",
    "        return articel_vec\n",
    "    \n",
    "    #计算P(w0|ci)和P(ci)\n",
    "    #改进1：为了避免P(w0|ci)中分子为0，导致整个P(w|ci)为0的情况，初始化occurrence为1\n",
    "    #改进2：因为计算出来的概率小，所有概率的乘积会更小，这样有可能会丢失精度，在这里对求出的概率取自然数对数处理\n",
    "    def __train_probility(self):\n",
    "        #train_matircs文章向量列表，train_vec是文章分类结果\n",
    "        train_matrics = []\n",
    "        for words in self.token_list:\n",
    "            train_matrics.append(self.bagOfWord2Vec(words))\n",
    "        train_vec = self.classVec\n",
    "        num_train_article = len(train_matrics)\n",
    "        num_word = len(train_matrics[0])\n",
    "        prob_c1 = sum(train_vec) / num_train_article\n",
    "        prob_w_c0 = np.ones(num_word)   #从zeros改为ones,初始化分子为1 改进1\n",
    "        prob_w_c1 = np.ones(num_word)   #从zeros改为ones，初始化分子为1 改进1\n",
    "        num_c0 = 2  #初始化分母从0改为2  改进1\n",
    "        num_c1 = 2  #初始化分母从0改为2  改进1\n",
    "        \n",
    "        #print(len(train_vec), num_train_article)\n",
    "        for i in range(num_train_article):\n",
    "            if train_vec[i] == 1:\n",
    "                prob_w_c1 += train_matrics[i]\n",
    "                num_c1 += sum(train_matrics[i])\n",
    "            else:\n",
    "                prob_w_c0 += train_matrics[i]\n",
    "                num_c0 += sum(train_matrics[i])                \n",
    "        prob1_vec = np.log(prob_w_c1/num_c1)\n",
    "        prob0_vec = np.log(prob_w_c0/num_c0)\n",
    "        \n",
    "        return prob0_vec, prob1_vec , prob_c1\n",
    "    \n",
    "    def classifyNB(self, input_article):\n",
    "        #因为转换为log所以和即是概率的乘积\n",
    "        input_vec = self.bagOfWord2Vec(input_article)\n",
    "        #print(input_vec)\n",
    "        #print(self.p1v)\n",
    "        #矩阵*表示单个单个元素对应相乘，维度要一样，因为p1v保存了所有P(wi|ci)，\n",
    "        #要求input_vec的P(wi|ci)，只要把p1v中属于input_vec的项提取出来即可\n",
    "        p1 = sum(input_vec * self.p1v) + np.log(self.pclass1) \n",
    "        p2 = sum(input_vec * self.p0v) + np.log(1-self.pclass1)     \n",
    "        if p1 > p2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 利用朴素贝叶斯对垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "def textParse(bigstring):\n",
    "    listOfTokens = re.split(r'\\W+',bigstring)\n",
    "    return [word.lower() for word in listOfTokens if len(word) > 2] #剔除长度小于2的词\n",
    "\n",
    "def spamtest():\n",
    "    doclist = []\n",
    "    classvec = []\n",
    "    path1 = '.\\email\\ham'\n",
    "    path2 = '.\\email\\spam'\n",
    "    for i in range(1,26):\n",
    "        #print(os.path.join(path1,'{}.txt'.format(i)))\n",
    "        word_list = textParse(open(os.path.join(path1,'{}.txt'.format(i))).read())    \n",
    "        doclist.append(word_list)\n",
    "        classvec.append(1)\n",
    "        word_list = textParse(open(os.path.join(path2,'{}.txt'.format(i))).read())\n",
    "        doclist.append(word_list)\n",
    "        classvec.append(0)  \n",
    "    num = len(classvec)\n",
    "    #随机选择一些数据做为测试数据，剩余的数据作为训练数据，该方法又称交叉验证\n",
    "    randomindex = random.sample(range(num), 10)\n",
    "    trainset = [article  for _i,article in enumerate(doclist) if _i not in randomindex]\n",
    "    trainsetclass = [a  for _i,a in enumerate(classvec) if _i not in randomindex]\n",
    "    \n",
    "    #testset = [article  for _i,article in enumerate(word_list) if _i in randomindex]\n",
    "    \n",
    "    my_bayes = Navie_Bayes(trainset, trainsetclass)\n",
    "    errorcount = 0\n",
    "    #print(num,len(word_list) , len(classvec),randomindex)\n",
    "    for i  in randomindex:\n",
    "        if my_bayes.classifyNB(doclist[i]) != classvec[i]:\n",
    "            errorcount +=1\n",
    "    \n",
    "    print('the error rate is {:6f}'.format(errorcount / len(randomindex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is 0.000000\n"
     ]
    }
   ],
   "source": [
    "spamtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#path = os.path.join('E:\\MYGIT\\Self-learning\\Machine Learning In Action\\Ch04\\email\\ham', '23.txt')\n",
    "#print(open(path, encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#listoftokens = list(re.split(r'\\W+', lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yay to you both doing fine I m working on an MBA in Design Strategy at CCA top art school It s a new program focusing on more of a right brained creative and strategic approach to management I m an 1 8 of the way done today '"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(listoftokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用朴素 贝叶斯对新闻分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './news/时政新闻.txt'\n",
    "path2 = './news/娱乐新闻.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    with open('stopwords.txt') as f:\n",
    "        linestr = f.readline()\n",
    "        while linestr != '':\n",
    "            stopwords.append(linestr.strip())\n",
    "            linestr = f.readline()  \n",
    "    return stopwords\n",
    "\n",
    "def textPrase_chinese(bigstring, stopwords):\n",
    "    bigstring = bigstring.strip().replace(' ','')\n",
    "    jieba_list = jieba.cut(bigstring)\n",
    "    words_list = []\n",
    "    for word in jieba_list:\n",
    "        if word not in stopwords:\n",
    "            words_list.append(word)\n",
    "    return  words_list   \n",
    "\n",
    "def news_classify_test():\n",
    "    time_point1 = time.time()\n",
    "    doclist = []\n",
    "    classvec = []\n",
    "    path1 = './news/时政新闻.txt'  #该新闻分类为1，如果多余两个类别，要对原来的Navie_Bayes_BOW函数的里预测进行修改\n",
    "    path2 = './news/娱乐新闻.txt'  #该新闻分类为0\n",
    "    stopwords = get_stopwords()\n",
    "    with open(path1, encoding='utf-8') as f:\n",
    "        linestr = f.readline()\n",
    "        for _i in range(500): #获取前500片文章\n",
    "            word_list = textPrase_chinese(linestr, stopwords)    \n",
    "            doclist.append(word_list)\n",
    "            classvec.append(1)\n",
    "            linestr = f.readline()\n",
    "    with open(path2, encoding='utf-8') as f:\n",
    "        linestr = f.readline()\n",
    "        for _i in range(500):#获取前500片文章\n",
    "            word_list = textPrase_chinese(linestr, stopwords)\n",
    "            doclist.append(word_list)\n",
    "            classvec.append(0) \n",
    "            linestr = f.readline()\n",
    "            \n",
    "    num = len(classvec)\n",
    "    #随机选择一些数据做为测试数据，剩余的数据作为训练数据，该方法又称交叉验证\n",
    "    randomindex = random.sample(range(num), 30) #随机生成10个不同数\n",
    "    trainset = [article  for _i,article in enumerate(doclist) if _i not in randomindex]\n",
    "    trainsetclass = [a  for _i,a in enumerate(classvec) if _i not in randomindex]\n",
    "    \n",
    "    #testset = [article  for _i,article in enumerate(word_list) if _i in randomindex]\n",
    "    time_point2 = time.time()\n",
    "    print('数据据预处理时间：{:4f}s'.format(time_point2 -time_point1))\n",
    "    \n",
    "    my_bayes = Navie_Bayes_BOW(trainset, trainsetclass)\n",
    "    errorcount = 0\n",
    "    \n",
    "    time_point3 = time.time()\n",
    "    #print(len(my_bayes.vacabulary))\n",
    "    print('模型建立时间：{:4f}s'.format(time_point3 -time_point2))\n",
    "    \n",
    "    #print(num,len(word_list) , len(classvec),randomindex)\n",
    "    for i  in randomindex:\n",
    "        if my_bayes.classifyNB(doclist[i]) != classvec[i]:\n",
    "            errorcount +=1\n",
    "    print('预测时间: {:4f}s'.format(time.time()-time_point3))\n",
    "    print('the error rate is {:4f}'.format(errorcount / len(randomindex)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据据预处理时间：18.704432s\n",
      "模型建立时间：5.007609s\n",
      "预测时间: 0.624001s\n",
      "the error rate is 0.033333\n"
     ]
    }
   ],
   "source": [
    "news_classify_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.random.rand(15000)\n",
    "bb = np.random.rand(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3732.475282787691"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum(aa * bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用sklearn来实现朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     5,
     14
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import random\n",
    "import time\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    with open('stopwords.txt') as f:\n",
    "        linestr = f.readline()\n",
    "        while linestr != '':\n",
    "            stopwords.append(linestr.strip())\n",
    "            linestr = f.readline()  \n",
    "    return stopwords\n",
    "\n",
    "def textPrase_chinese(bigstring, stopwords):\n",
    "    bigstring = bigstring.strip().replace(' ','')\n",
    "    jieba_list = jieba.cut(bigstring)\n",
    "    words_list = []\n",
    "    for word in jieba_list:\n",
    "        if word not in stopwords:\n",
    "            words_list.append(word)\n",
    "    return  words_list   \n",
    "\n",
    "def get_dataset():\n",
    "    doclist = []\n",
    "    classvec = []\n",
    "    path1 = './news/时政新闻.txt'  #该新闻分类为1，如果多余两个类别，要对原来的Navie_Bayes_BOW函数的里预测进行修改\n",
    "    path2 = './news/娱乐新闻.txt'  #该新闻分类为0\n",
    "    stopwords = get_stopwords()\n",
    "    with open(path1, encoding='utf-8') as f:\n",
    "        linestr = f.readline()\n",
    "        for _i in range(500): #获取前500片文章\n",
    "            word_list = textPrase_chinese(linestr, stopwords)    \n",
    "            doclist.append(word_list)\n",
    "            classvec.append(1)\n",
    "            linestr = f.readline()\n",
    "    with open(path2, encoding='utf-8') as f:\n",
    "        linestr = f.readline()\n",
    "        for _i in range(500):#获取前500片文章\n",
    "            word_list = textPrase_chinese(linestr, stopwords)\n",
    "            doclist.append(word_list)\n",
    "            classvec.append(0) \n",
    "            linestr = f.readline()\n",
    "            \n",
    "    return doclist, classvec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用词袋模型来获取词向量，不进行word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_of_dataset(dataset):\n",
    "    tokens = []\n",
    "    for words in dataset:\n",
    "        tokens += words \n",
    "    #移除高频词，因为高频词大多数都是连接等关系，等于语意无关的词，如果停用次比较全可以不这样处理\n",
    "    #token_count = Counter(tokens)\n",
    "    #token_count_sorted = list(sorted(token_count.items() , key=lambda x:x[1], reverse=True))\n",
    "    #tokens = [token[1] for token in token_count_sorted[20:]]\n",
    "    tokens = set(tokens)\n",
    "    #tokens = list(tokens)\n",
    "    tokens_dict = {} #主要是为了防止每次使用list.index来获取索引，导致程序运行时间变长\n",
    "    for i,  word in enumerate(tokens):\n",
    "        tokens_dict[word]= i\n",
    "    print(len(tokens))\n",
    "    \n",
    "    #获取inputset的向量值，1表示该词出现\n",
    "    articel_vec = np.zeros((len(dataset), len(tokens)))\n",
    "    timecount = 0\n",
    "    for i in range(len(dataset)):\n",
    "        for word in dataset[i]:\n",
    "            if word in tokens: #如果tokens是列表的话，这里性能最少降低十几倍\n",
    "                articel_vec[i][tokens_dict[word]] += 1 # =1 变为 +=1 \n",
    "            else:\n",
    "                pass\n",
    "                #print('the word {} is not in my vocabulary'.format(word))\n",
    "    return articel_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,y = get_dataset()#两种类别文章各500篇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42613\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = get_vec_of_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(970, 41768)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "def Naive_model_Gaussion(X,y,test_rate=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split\\\n",
    "    (X, y, test_size=test_rate, random_state=0)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    predict_prob_y = gnb.predict(X_test)\n",
    "    print(classification_report(y_test, predict_prob_y))\n",
    "# Multinomial Naive Bayes   \n",
    "def Naive_model_Multinomial(X,y,test_rate=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split\\\n",
    "    (X, y, test_size=test_rate, random_state=0)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict_prob_y = clf.predict(X_test)\n",
    "    print(classification_report(y_test, predict_prob_y))\n",
    "# Bernoulli Naive Bayes  \n",
    "def Naive_model_Bernoulli(X,y,test_rate=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split\\\n",
    "    (X, y, test_size=test_rate, random_state=0)\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict_prob_y = clf.predict(X_test)\n",
    "    print(classification_report(y_test, predict_prob_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       148\n",
      "           1       1.00      0.91      0.95       143\n",
      "\n",
      "    accuracy                           0.96       291\n",
      "   macro avg       0.96      0.95      0.96       291\n",
      "weighted avg       0.96      0.96      0.96       291\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       148\n",
      "           1       0.99      1.00      1.00       143\n",
      "\n",
      "    accuracy                           1.00       291\n",
      "   macro avg       1.00      1.00      1.00       291\n",
      "weighted avg       1.00      1.00      1.00       291\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89       148\n",
      "           1       0.83      1.00      0.91       143\n",
      "\n",
      "    accuracy                           0.90       291\n",
      "   macro avg       0.91      0.90      0.90       291\n",
      "weighted avg       0.91      0.90      0.90       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Naive_model_Gaussion(X,y)\n",
    "Naive_model_Multinomial(X,y)\n",
    "Naive_model_Bernoulli(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面测试，不难看出，Gaussian Naive Bayes在对文本二类分类效果比较好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用TF-IDF来构建文本向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = TfidfVectorizer(max_features = 20000) #设置文本单词个数最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = [' '.join(words) for words  in dataset]\n",
    "X1 = vectorized.fit_transform(dataset_new).toarray()\n",
    "y1 = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(970, 41768)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       148\n",
      "           1       0.99      0.91      0.95       143\n",
      "\n",
      "    accuracy                           0.95       291\n",
      "   macro avg       0.96      0.95      0.95       291\n",
      "weighted avg       0.95      0.95      0.95       291\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       148\n",
      "           1       0.99      0.98      0.99       143\n",
      "\n",
      "    accuracy                           0.99       291\n",
      "   macro avg       0.99      0.99      0.99       291\n",
      "weighted avg       0.99      0.99      0.99       291\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       148\n",
      "           1       0.88      1.00      0.94       143\n",
      "\n",
      "    accuracy                           0.93       291\n",
      "   macro avg       0.94      0.94      0.93       291\n",
      "weighted avg       0.94      0.93      0.93       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Naive_model_Gaussion(X1,y1)\n",
    "Naive_model_Multinomial(X1,y1)\n",
    "Naive_model_Bernoulli(X1,y1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
