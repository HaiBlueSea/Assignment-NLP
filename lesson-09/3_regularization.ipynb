{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T07:05:14.286077Z",
     "start_time": "2019-09-06T07:05:11.899940Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T07:05:43.700759Z",
     "start_time": "2019-09-06T07:05:42.634698Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (8984, 28, 28) (8984,)\n",
      "Test set (8719, 28, 28) (8719,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_root = 'E:/MYGIT/DataSources/notMNIST'\n",
    "pickle_file = os.path.join(data_root, 'notMNIST_remove_oevrlap.pickle')\n",
    "pickle_file_1 = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del(save)  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T07:06:03.734905Z",
     "start_time": "2019-09-06T07:06:03.399886Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (8984, 784) (8984, 10)\n",
      "Test set (8719, 784) (8719, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T07:06:25.383143Z",
     "start_time": "2019-09-06T07:06:25.314140Z"
    }
   },
   "outputs": [],
   "source": [
    "np.argmax??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T09:51:28.568618Z",
     "start_time": "2019-09-06T09:51:28.429610Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "## Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic model add L2\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T10:10:54.580310Z",
     "start_time": "2019-09-06T10:10:53.694259Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))##随机梯度参数设置不同之处\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))##随机梯度参数设置不同之处\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)##L2参数设置\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                          (labels=tf_train_labels, logits=logits))beta_regul*tf.nn.l2_loss(weights)\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T10:13:34.674467Z",
     "start_time": "2019-09-06T10:13:23.292816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.505033\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 2.375235\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 1000: 1.801144\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1500: 1.527068\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 2000: 1.063528\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2500: 0.587298\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 3000: 0.811859\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.8%\n",
      "Test accuracy: 87.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "##3001*128 总共的训练数据   ## 之前是801*10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        ##分母减去batch_size是防止当batch_size不能被训练数据量整除时，offset:(offset + batch_size)超出数组界限\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T10:17:21.374433Z",
     "start_time": "2019-09-06T10:15:29.475033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#测试L2正则系数不同时，ACC的表现\n",
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        #f.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T11:23:57.713011Z",
     "start_time": "2019-09-06T11:23:57.492998Z"
    }
   },
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T11:26:02.172129Z",
     "start_time": "2019-09-06T11:26:01.907114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test accuracy by regularization (logistic)')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural network add L2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T11:33:39.862308Z",
     "start_time": "2019-09-06T11:33:39.319277Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32) ##addL2\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))##隐藏层1024个节点\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.##设置神经网络tf.nn.relu()里的参数相当于逻辑回归的函数logits\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))##add L2\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1) \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T11:34:39.742733Z",
     "start_time": "2019-09-06T11:33:42.389452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 636.012085\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 27.1%\n",
      "Minibatch loss at step 500: 193.020981\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1000: 113.177940\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1500: 68.721069\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2000: 41.171753\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2500: 25.151037\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3000: 15.502783\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "##3001*128 总共的训练数据   ## 之前是801*10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:09:48.908370Z",
     "start_time": "2019-09-06T11:50:30.000084Z"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:09:59.948002Z",
     "start_time": "2019-09-06T12:09:59.690987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test accuracy by regularization (1-layer net)')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "## Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:10:29.130671Z",
     "start_time": "2019-09-06T12:10:28.610641Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32) ##addL2\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))##隐藏层1024个节点\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.##设置神经网络tf.nn.relu()里的参数相当于逻辑回归的函数logits\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \n",
    "    #+beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))##add L2\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1) \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:10:45.681617Z",
     "start_time": "2019-09-06T12:10:37.279137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 333.795166\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 23.8%\n",
      "Minibatch loss at step 10: 24.655172\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 20: 6.236622\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Test accuracy: 79.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        ##这里把相当于固定很小一部分的训练数据，然后一直训练这部分数据，所以可得到过拟合现象\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:10:57.821312Z",
     "start_time": "2019-09-06T12:10:57.274281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-120482c074b4>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32) ##addL2\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))##隐藏层1024个节点\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.##设置神经网络tf.nn.relu()里的参数相当于逻辑回归的函数logits\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5) #add drop\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \n",
    "    #+beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))##add L2\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1) \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:11:53.564500Z",
     "start_time": "2019-09-06T12:11:45.362031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 518.917480\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 29.7%\n",
      "Minibatch loss at step 10: 61.548069\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 20: 21.000814\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 30: 4.510040\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 40: 1.190803\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 50: 2.163004\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 60: 0.359124\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 70: 1.784384\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 80: 1.012117\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 90: 3.897972\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 100: 0.481134\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.9%\n",
      "Test accuracy: 81.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        ##这里把相当于固定很小一部分的训练数据，然后一直训练这部分数据，所以可得到过拟合现象\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    tf.train.exponential_decay(\n",
    "    learning_rate,\n",
    "    global_step,\n",
    "    decay_steps,\n",
    "    decay_rate,\n",
    "    staircase=False,\n",
    "    name=None,)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:18:50.210331Z",
     "start_time": "2019-09-06T12:18:50.179329Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.train.exponential_decay??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:13:57.267576Z",
     "start_time": "2019-09-06T12:13:57.233574Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.truncated_normal??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:20:31.354116Z",
     "start_time": "2019-09-06T12:20:30.740081Z"
    }
   },
   "outputs": [],
   "source": [
    "#2层神经网络\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    #第一层输入节点是28*28级原始数据的维度大小，本层节点是个数是num_hidden_nodes1\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes1],\n",
    "                                               stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "    #第二层的输入节点是第一层节点个数，本层节点个数是num_hidden_nodes2,stddev是指定生成数据的标准差\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    #最后一层的输入节点个数是第二层节点个数，本层节点个数是要分类的类别个数。\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:24:12.298753Z",
     "start_time": "2019-09-06T12:20:59.914750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.200811\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 40.3%\n",
      "Minibatch loss at step 500: 0.949447\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1000: 0.794478\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 1500: 0.860234\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2000: 0.687163\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2500: 0.465837\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3000: 0.611498\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3500: 0.473561\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.445982\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4500: 0.421937\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 5000: 0.509907\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5500: 0.392359\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6000: 0.499359\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 6500: 0.441872\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7000: 0.409310\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7500: 0.333621\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.425128\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8500: 0.439213\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.432493\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Test accuracy: 95.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:39:13.454296Z",
     "start_time": "2019-09-06T12:39:12.833261Z"
    }
   },
   "outputs": [],
   "source": [
    "#三层神经网络\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable( tf.truncated_normal([image_size * image_size, num_hidden_nodes1], \n",
    "                                                stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, \n",
    "                                                num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_hidden_nodes3, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "    logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+ tf.nn.l2_loss(weights4))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:46:56.962808Z",
     "start_time": "2019-09-06T12:39:15.500413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.511766\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 23.6%\n",
      "Minibatch loss at step 500: 1.095654\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1000: 0.874152\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1500: 0.890295\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2000: 0.678811\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 0.467644\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 0.599171\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3500: 0.480820\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 4000: 0.464675\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 4500: 0.366282\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 5000: 0.516457\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 5500: 0.384022\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 6000: 0.522545\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 6500: 0.435266\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.367603\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 7500: 0.342508\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8000: 0.423467\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 8500: 0.419438\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9000: 0.385070\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9500: 0.590286\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 10000: 0.513061\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 10500: 0.392096\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 11000: 0.362965\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 11500: 0.498055\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12000: 0.478680\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12500: 0.435049\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 13000: 0.339808\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 13500: 0.416174\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 14000: 0.453454\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 14500: 0.364158\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 15000: 0.299470\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 15500: 0.378149\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 16000: 0.285843\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 16500: 0.367406\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 17000: 0.296379\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 17500: 0.379118\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 18000: 0.336206\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Test accuracy: 95.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:49:18.957929Z",
     "start_time": "2019-09-06T12:49:18.199886Z"
    }
   },
   "outputs": [],
   "source": [
    "###3层神经网络+随机失活(dropout)\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable( tf.truncated_normal([image_size * image_size, num_hidden_nodes1], \n",
    "                                                stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, \n",
    "                                                num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_hidden_nodes3, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "    drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "    drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "    logits = tf.matmul(drop3, weights4) + biases4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) #+ \\\n",
    "     # beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+tf.nn.l2_loss(weights4))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T12:58:09.792291Z",
     "start_time": "2019-09-06T12:49:21.772090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.742796\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 500: 0.517210\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1000: 0.614306\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1500: 0.636285\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2000: 0.584990\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2500: 0.401859\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 0.589148\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3500: 0.453759\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4000: 0.444367\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4500: 0.419620\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 5000: 0.508793\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.305553\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6000: 0.515476\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 6500: 0.466925\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 7000: 0.422220\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7500: 0.291298\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8000: 0.394093\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8500: 0.465321\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 9000: 0.409750\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9500: 0.556016\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10000: 0.581417\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 10500: 0.384624\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 11000: 0.271278\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 11500: 0.515202\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12000: 0.401201\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 12500: 0.434951\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 13000: 0.312331\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 13500: 0.392783\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 14000: 0.489375\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 14500: 0.299089\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15000: 0.266828\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15500: 0.373695\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 16000: 0.329973\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 16500: 0.357475\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17000: 0.262581\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 17500: 0.418104\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 18000: 0.338831\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 18500: 0.191776\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 19000: 0.317760\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 19500: 0.378309\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 20000: 0.492805\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Test accuracy: 94.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "step_save = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        step_save.append(global_step)\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
