{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a neural network from sractch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/fc_layer.html\n",
    "$$(\\begin{bmatrix}\n",
    "       x_{1} & x_{2} & x_{3} \\end{bmatrix}.\\begin{bmatrix}\n",
    "       w_{11} & w_{21} \\\\\n",
    "       w_{12} & w_{22} \\\\\n",
    "       w_{13} & w_{23}\n",
    "     \\end{bmatrix})+\\begin{bmatrix}\n",
    "       b_{1} & b_{2}\\end{bmatrix}=\\begin{bmatrix}\n",
    "       y_{1} & y_{2}\\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X}=\\begin{bmatrix}\n",
    "       dout_{y1} & dout_{y2} \\end{bmatrix}.\\begin{bmatrix}\n",
    "       w_{11} & w_{12} & w_{13} \\\\\n",
    "       w_{21} & w_{22} & w_{23}        \n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "W=\\begin{bmatrix}\n",
    "       w_{11} & w_{12} & w_{13} \\\\\n",
    "       w_{21} & w_{22} & w_{23}        \n",
    "     \\end{bmatrix} \\therefore \\frac{\\partial L}{\\partial W}=\\begin{bmatrix}\n",
    "       \\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}} \\\\\n",
    "       \\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}}        \n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}=\\begin{bmatrix}\n",
    "       dout_{y1} \\\\ dout_{y2} \\end{bmatrix}.\\begin{bmatrix}\n",
    "       x_{1} && x_{2} && x_{3}\n",
    "     \\end{bmatrix}=\\begin{bmatrix}\n",
    "       \\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}} \\\\\n",
    "       \\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}}        \n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}=\\begin{bmatrix}\n",
    "       dout_{y1} & dout_{y2} \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Each node in neural networks will have these attributes and methods\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=[]):\n",
    "        \"\"\"\n",
    "        if the node is the operator of \"ax + b\", the inputs will be x node , and the outputs \n",
    "        of this is its successors. so inputs and outputs store the connected node object\n",
    "\n",
    "        and the value is *ax + b*\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        self.value = None\n",
    "        self.gradients ={}\n",
    "        \n",
    "        for node in self.inputs:\n",
    "            node.outputs.append(self) #build connected relationship\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        compute the output value based on the input nodes\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        compute the gradients of current note based on the input nodes\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    '''\n",
    "    if the node is the operator of \"ax + b\"; see a , b , x both as a node object\n",
    "    So this Input object is for feedding data to the layer.\n",
    "    In order to make graph simple ,and casue it's fully connected network ,we can see one layer as as Node\n",
    "    '''\n",
    "    def __init__(self, name, inputs=[]):\n",
    "        self.name = name\n",
    "        super().__init__(inputs)\n",
    "    \n",
    "    def forward(self, value= None):\n",
    "        if value:         \n",
    "            self.value = value\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradients = {}\n",
    "        \n",
    "        #从输出节点中获取loss函数对该节点的偏导，而用该值乘该节点对input参数的导数即得到loss对input的导数\n",
    "        #由此可见该节点可以得到loss对input的导数，这也是为什么该节点直接从output节点中获取loss函数对该节点的偏导\n",
    "        for node in self.outputs:\n",
    "            grad_cost = node.gradients[self]\n",
    "            self.gradients[self] = grad_cost        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Input Node: {}'.format(self.name)   \n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        '''\n",
    "        nodes, weigths,bias is Input node object\n",
    "        '''\n",
    "        self.w_node = weights\n",
    "        self.x_node = nodes\n",
    "        self.b_node = bias     \n",
    "        super().__init__(inputs=[nodes, weights, bias])\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        compute the a*x + b, In order to make it available for matric input ,\n",
    "        we use numpy  to caculate    \n",
    "        '''\n",
    "        #print(self.x_node.value.shape, self.w_node.value.shape,'33333')\n",
    "        self.value = np.dot(self.x_node.value, self.w_node.value.T) + self.b_node.value\n",
    "        \n",
    "    def backward(self):\n",
    "        '''\n",
    "        '''\n",
    "        for node in self.outputs:\n",
    "            grad_cost = node.gradients[self]\n",
    "            self.gradients[self] = grad_cost     \n",
    "            self.gradients[self.w_node] = np.dot(grad_cost.T,self.x_node.value)\n",
    "            self.gradients[self.b_node] = grad_cost\n",
    "            self.gradients[self.x_node] = np.dot(grad_cost, self.w_node.value)\n",
    "            \n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        self.x_node = node\n",
    "        super().__init__([node])\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x_node.value)\n",
    "    \n",
    "    def backward(self):\n",
    "        y = self.value   \n",
    "        self.partial = y * (1 - y)\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost\n",
    "            #不管是矩阵输入还是单个输入，self.partial都是一个一维常数，所以可以直接相乘\n",
    "            self.gradients[self.x_node] = grad_cost * self.partial\n",
    "            \n",
    "class MSE(Node):\n",
    "    def __init__(self,y_true, y_pre):\n",
    "        self.y_true_node = y_true\n",
    "        self.y_pre_node = y_pre\n",
    "        super().__init__(inputs=[y_pre,y_true])\n",
    "    \n",
    "    def forward(self):\n",
    "        y_true_flatten  = self.y_true_node.value.reshape(-1,1)\n",
    "        y_pre_flatten = self.y_pre_node.value.reshape(-1,1)\n",
    "        self.diff = y_true_flatten - y_pre_flatten\n",
    "        self.value = np.mean(self.diff**2)\n",
    "    \n",
    "    def backward(self):\n",
    "        n = self.y_pre_node.value.shape[0]\n",
    "        self.gradients[self.y_true_node] = (2 / n) * self.diff\n",
    "        self.gradients[self.y_pre_node] =  (-2 / n) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_one_batch(topological_sorted_graph):\n",
    "    # graph 是经过拓扑排序之后的 一个list\n",
    "    for node in topological_sorted_graph:\n",
    "        #print(node)\n",
    "        node.forward()\n",
    "        \n",
    "    for node in topological_sorted_graph[::-1]:\n",
    "        node.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0.9999991684719722\n",
      "0.4899988358614525\n"
     ]
    }
   ],
   "source": [
    "#单节点传播测试\n",
    "#当输入数据变成矩阵时，相当于单节点就变成了layer\n",
    "step1 = 3*4+2\n",
    "print(step1)\n",
    "step2 = 1. / (1 + np.exp(-1 * step1))\n",
    "print(step2)\n",
    "loss1 = (step2-0.3)**2\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input('x')\n",
    "y = Input('y')\n",
    "w = Input('w')\n",
    "b = Input('b')\n",
    "linear_output = Linear(x, w, b)\n",
    "yhat = Sigmoid(linear_output)\n",
    "loss = MSE(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value = np.array([3])\n",
    "y.value = np.array([0.3])\n",
    "w.value = np.array([4])\n",
    "b.value = np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output.forward()\n",
    "yhat.forward()\n",
    "loss.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "yhat.backward()\n",
    "linear_output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] [0.99999917] 0.4899988358614525\n"
     ]
    }
   ],
   "source": [
    "print(linear_output.value, yhat.value, loss.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(data_with_value):\n",
    "    input_nodes = [n for n in data_with_value.keys()]\n",
    "    \n",
    "    nodes = input_nodes.copy()\n",
    "    G = {}\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0) #广度优先搜索找出每个节点的in节点和out节点\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "#         if isinstance(n, Input):\n",
    "#             n.value = feed_dict[n] #feed orgin value, it's ok do it somewhere else        \n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(trainable_nodes, learning_rate=1e-2):\n",
    "    '''\n",
    "    因为我们要优化的是w和b这些参数矩阵，所以trainable_nodes就应该是这些参数代表的节点\n",
    "    '''\n",
    "    for t in trainable_nodes:\n",
    "        #print(t, t.gradients[t].shape, t.value.shape)\n",
    "        step = 1 * learning_rate * t.gradients[t]\n",
    "        t.value = t.value - step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orgin data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "n_features = X_.shape[1]\n",
    "n_hidden_1 = 64 #定义一个隐藏层是64个节点\n",
    "n_hidden_2 = 64 #定义第二隐藏层是64个节点\n",
    "n_out  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_, b1_ = np.random.randn(n_hidden_1, n_features), np.zeros(n_hidden_1) #这里矩阵形状要根据Linear节点里面计算方式匹配\n",
    "W2_, b2_ = np.random.randn(n_hidden_2, n_features), np.zeros(n_hidden_2)\n",
    "W3_, b3_ = np.random.randn(1, n_features), np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 13)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build node \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Input(name='X'), Input(name='y')  # tensorflow -> placeholder\n",
    "W1, b1 = Input(name='W1'), Input(name='b1')\n",
    "W2, b2 = Input(name='W2'), Input(name='b2')\n",
    "W3, b3 = Input(name='W3'), Input(name='b3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bulid node connected relationship\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output1 = Linear(X, W1, b1) # 首层网络\n",
    "sigmoid_output1 = Sigmoid(linear_output1) #sigmid处理\n",
    "linear_output2 = Linear(sigmoid_output1, W2, b2) #第二层\n",
    "sigmoid_output2 = Sigmoid(linear_output2) ##sigmid处理\n",
    "yhat = Linear(sigmoid_output2, W3, b3) #输出层\n",
    "loss = MSE(y, yhat)  #最后一层，实际上这层只是用来计算loss的，在我们学习的网络层中，这层并没有，没有体现出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_output1.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get topological_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node_with_value = {  # \n",
    "    X: X_, \n",
    "    y: y_, \n",
    "    W1: W1_, \n",
    "    W2: W2_, \n",
    "    W2: W2_,\n",
    "    b1: b1_, \n",
    "    b2: b2_,\n",
    "    b3: b3_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Input Node: b3,\n",
       " Input Node: y,\n",
       " Input Node: b1,\n",
       " Input Node: W2,\n",
       " Input Node: W1,\n",
       " Input Node: X,\n",
       " Input Node: b2,\n",
       " <__main__.Linear at 0xf021ac8>,\n",
       " <__main__.Sigmoid at 0xf021550>,\n",
       " <__main__.Linear at 0xf021b70>,\n",
       " <__main__.Sigmoid at 0xf021da0>,\n",
       " <__main__.Linear at 0xf021c88>,\n",
       " <__main__.MSE at 0xf0210f0>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = topological_sort(input_node_with_value) #获得拓扑排序\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_data(feed_dict):\n",
    "    for key in feed_dict.keys():\n",
    "        key.value = feed_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed data\n",
    "feed_dict = {\n",
    "    W1: W1_, \n",
    "    W2: W2_, \n",
    "    W2: W2_,\n",
    "    b1: b1_, \n",
    "    b2: b2_,\n",
    "    b3: b3_\n",
    "}\n",
    "feed_data(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 229.548\n",
      "Epoch: 101, loss = 5.911\n",
      "Epoch: 201, loss = 4.100\n",
      "Epoch: 301, loss = 3.128\n",
      "Epoch: 401, loss = 2.386\n",
      "Epoch: 501, loss = 2.020\n",
      "Epoch: 601, loss = 2.099\n",
      "Epoch: 701, loss = 1.815\n",
      "Epoch: 801, loss = 1.434\n",
      "Epoch: 901, loss = 1.281\n",
      "Epoch: 1001, loss = 1.183\n",
      "Epoch: 1101, loss = 1.106\n",
      "Epoch: 1201, loss = 1.170\n",
      "Epoch: 1301, loss = 1.276\n",
      "Epoch: 1401, loss = 0.986\n",
      "Epoch: 1501, loss = 1.067\n",
      "Epoch: 1601, loss = 0.855\n",
      "Epoch: 1701, loss = 0.719\n",
      "Epoch: 1801, loss = 0.786\n",
      "Epoch: 1901, loss = 0.707\n",
      "Epoch: 2001, loss = 0.622\n",
      "Epoch: 2101, loss = 0.666\n",
      "Epoch: 2201, loss = 0.733\n",
      "Epoch: 2301, loss = 0.587\n",
      "Epoch: 2401, loss = 0.464\n",
      "Epoch: 2501, loss = 0.395\n",
      "Epoch: 2601, loss = 0.434\n",
      "Epoch: 2701, loss = 0.472\n",
      "Epoch: 2801, loss = 0.419\n",
      "Epoch: 2901, loss = 0.358\n",
      "Epoch: 3001, loss = 0.439\n",
      "Epoch: 3101, loss = 0.384\n",
      "Epoch: 3201, loss = 0.378\n",
      "Epoch: 3301, loss = 0.284\n",
      "Epoch: 3401, loss = 0.313\n",
      "Epoch: 3501, loss = 0.316\n",
      "Epoch: 3601, loss = 0.350\n",
      "Epoch: 3701, loss = 0.336\n",
      "Epoch: 3801, loss = 0.287\n",
      "Epoch: 3901, loss = 0.274\n",
      "Epoch: 4001, loss = 0.236\n",
      "Epoch: 4101, loss = 0.195\n",
      "Epoch: 4201, loss = 0.197\n",
      "Epoch: 4301, loss = 0.231\n",
      "Epoch: 4401, loss = 0.218\n",
      "Epoch: 4501, loss = 0.212\n",
      "Epoch: 4601, loss = 0.166\n",
      "Epoch: 4701, loss = 0.186\n",
      "Epoch: 4801, loss = 0.210\n",
      "Epoch: 4901, loss = 0.198\n"
     ]
    }
   ],
   "source": [
    "#def mytest():\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden_1 = 64 #定义一个隐藏层是64个节点\n",
    "n_hidden_2 = 64 #定义第二隐藏层是64个节点\n",
    "n_out  = 1\n",
    "\n",
    "W1_, b1_ = np.random.randn(n_hidden_1, n_features), np.zeros(n_hidden_1) #这里矩阵形状要根据Linear节点里面计算方式匹配\n",
    "W2_, b2_ = np.random.randn(n_hidden_2, n_hidden_1), np.zeros(n_hidden_2)\n",
    "W3_, b3_ = np.random.randn(1, n_hidden_2), np.zeros(1)\n",
    "\n",
    "# Input data. For the training data\n",
    "X, y = Input(name='X'), Input(name='y')  # tensorflow -> placeholder\n",
    "W1, b1 = Input(name='W1'), Input(name='b1')\n",
    "W2, b2 = Input(name='W2'), Input(name='b2')\n",
    "W3, b3 = Input(name='W3'), Input(name='b3')\n",
    "\n",
    "#network connection \n",
    "linear_output1 = Linear(X, W1, b1) # 首层网络\n",
    "sigmoid_output1 = Sigmoid(linear_output1) #sigmid处理\n",
    "linear_output2 = Linear(sigmoid_output1, W2, b2)#第二层\n",
    "sigmoid_output2 = Sigmoid(linear_output2) ##sigmid处理\n",
    "yhat = Linear(sigmoid_output2, W3, b3) #输出层\n",
    "loss = MSE(y, yhat)  #最后一层，实际上这层只是用来计算loss的，在我们学习的网络层中，这层并没有，没有体现出来\n",
    "\n",
    "# get topological sort, use this order to  compute Back propagation \n",
    "input_node_with_value = {X: X_, y: y_, W1: W1_, W2: W2_, W3: W3_,b1: b1_, b2: b2_,b3: b3_}\n",
    "graph = topological_sort(input_node_with_value) #获得拓扑排序\n",
    "\n",
    "#feed initial weight and bias of each layer\n",
    "feed_dict = {W1: W1_, W2: W2_, W3: W3_, b1: b1_, b2: b2_,b3: b3_}\n",
    "feed_data(feed_dict)\n",
    "#print(len(W1.value),'11111')\n",
    "\n",
    "losses = []\n",
    "epochs = 5000\n",
    "batch_size = 96\n",
    "steps_per_epoch = X_.shape[0] // batch_size\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for _step in range(steps_per_epoch):\n",
    "        #random choose data \n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "        #feed trainning data\n",
    "        feed_dict = {X:X_batch, y:y_batch}\n",
    "        feed_data(feed_dict)\n",
    "        #print(len(X.value),'222222')\n",
    "\n",
    "        training_one_batch(graph) #训练一次\n",
    "        sgd_update(trainable_nodes=[W1, W2, W3, b1, b2,b3], learning_rate=learning_rate) #根据训练结果更新数据一次\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch: {}, loss = {:.3f}'.format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17bae358>]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 64)\n",
      "(96, 64)\n",
      "(96, 64)\n"
     ]
    }
   ],
   "source": [
    "print(linear_output1.value.shape)\n",
    "print(sigmoid_output1.value.shape)\n",
    "print(linear_output2.value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import numpy as np\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.sigmoid),\n",
    "    tf.keras.layers.Dense(64,  activation=tf.nn.sigmoid),\n",
    "    tf.keras.layers.Dense(1,)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.01))#0.01步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "506/506 [==============================] - 1s 1ms/sample - loss: 502.9251\n",
      "Epoch 2/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 325.2068\n",
      "Epoch 3/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 238.2581\n",
      "Epoch 4/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 181.6791\n",
      "Epoch 5/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 142.2589\n",
      "Epoch 6/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 115.2059\n",
      "Epoch 7/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 97.8622\n",
      "Epoch 8/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 85.6501\n",
      "Epoch 9/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 75.4885\n",
      "Epoch 10/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 68.5290\n",
      "Epoch 11/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 64.5528\n",
      "Epoch 12/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 62.68 - 0s 57us/sample - loss: 61.5440\n",
      "Epoch 13/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 59.6841\n",
      "Epoch 14/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 58.0316\n",
      "Epoch 15/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 56.8586\n",
      "Epoch 16/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 55.7708\n",
      "Epoch 17/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 41.54 - 0s 61us/sample - loss: 54.8726\n",
      "Epoch 18/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 54.0136\n",
      "Epoch 19/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 57.29 - 0s 42us/sample - loss: 53.1110\n",
      "Epoch 20/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 55.89 - 0s 67us/sample - loss: 52.1832\n",
      "Epoch 21/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 51.2216\n",
      "Epoch 22/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 50.2744\n",
      "Epoch 23/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 49.1820\n",
      "Epoch 24/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 48.1620\n",
      "Epoch 25/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 47.1018\n",
      "Epoch 26/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 46.2144\n",
      "Epoch 27/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 44.87 - 0s 73us/sample - loss: 45.0095\n",
      "Epoch 28/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 43.7232\n",
      "Epoch 29/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 42.5222\n",
      "Epoch 30/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 41.2175\n",
      "Epoch 31/500\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 39.2835\n",
      "Epoch 32/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 37.5444\n",
      "Epoch 33/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 35.8706\n",
      "Epoch 34/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 34.2479\n",
      "Epoch 35/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 32.9041\n",
      "Epoch 36/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 33.69 - 0s 63us/sample - loss: 31.6531\n",
      "Epoch 37/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 30.1147\n",
      "Epoch 38/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 28.6093\n",
      "Epoch 39/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 27.3888\n",
      "Epoch 40/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 26.2112\n",
      "Epoch 41/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 24.9061\n",
      "Epoch 42/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 24.0391\n",
      "Epoch 43/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 23.0000\n",
      "Epoch 44/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 21.8794\n",
      "Epoch 45/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 21.0180\n",
      "Epoch 46/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 20.2595\n",
      "Epoch 47/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 19.5105\n",
      "Epoch 48/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 18.9557\n",
      "Epoch 49/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 18.4402\n",
      "Epoch 50/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 18.0450\n",
      "Epoch 51/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 17.7934\n",
      "Epoch 52/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 17.2068\n",
      "Epoch 53/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 16.7327\n",
      "Epoch 54/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 16.2239\n",
      "Epoch 55/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 16.1033\n",
      "Epoch 56/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 15.8708\n",
      "Epoch 57/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 15.3362\n",
      "Epoch 58/500\n",
      "506/506 [==============================] - 0s 97us/sample - loss: 15.2185\n",
      "Epoch 59/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 14.8335\n",
      "Epoch 60/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 14.5577\n",
      "Epoch 61/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 14.2785\n",
      "Epoch 62/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 13.9696\n",
      "Epoch 63/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 13.8906\n",
      "Epoch 64/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 12.27 - 0s 36us/sample - loss: 13.7066\n",
      "Epoch 65/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 13.4378\n",
      "Epoch 66/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 13.0920\n",
      "Epoch 67/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 12.8382\n",
      "Epoch 68/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 12.7831\n",
      "Epoch 69/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 12.2978\n",
      "Epoch 70/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 12.2164\n",
      "Epoch 71/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 11.8499\n",
      "Epoch 72/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 11.9723\n",
      "Epoch 73/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 11.8605\n",
      "Epoch 74/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 11.2958\n",
      "Epoch 75/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 11.1864\n",
      "Epoch 76/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 10.8289\n",
      "Epoch 77/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 10.4934\n",
      "Epoch 78/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 10.5118\n",
      "Epoch 79/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 10.2547\n",
      "Epoch 80/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 6.149 - 0s 40us/sample - loss: 10.0577\n",
      "Epoch 81/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 9.8744\n",
      "Epoch 82/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 9.7640\n",
      "Epoch 83/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 9.5708\n",
      "Epoch 84/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 9.5442\n",
      "Epoch 85/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 9.4298\n",
      "Epoch 86/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 9.1355\n",
      "Epoch 87/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 8.8513\n",
      "Epoch 88/500\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 8.8329\n",
      "Epoch 89/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 8.6141\n",
      "Epoch 90/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 8.5712\n",
      "Epoch 91/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 8.1556\n",
      "Epoch 92/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 8.0377\n",
      "Epoch 93/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 43us/sample - loss: 7.8674\n",
      "Epoch 94/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 7.9813\n",
      "Epoch 95/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 7.8595\n",
      "Epoch 96/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 7.3910\n",
      "Epoch 97/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 7.7433\n",
      "Epoch 98/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 7.8936\n",
      "Epoch 99/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 7.4276\n",
      "Epoch 100/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 7.2933\n",
      "Epoch 101/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 7.1383\n",
      "Epoch 102/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 6.9114\n",
      "Epoch 103/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 8.720 - 0s 43us/sample - loss: 6.9249\n",
      "Epoch 104/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 6.5108\n",
      "Epoch 105/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 6.8435\n",
      "Epoch 106/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 6.4827\n",
      "Epoch 107/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 6.3100\n",
      "Epoch 108/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 6.2786\n",
      "Epoch 109/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 6.1022\n",
      "Epoch 110/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 6.0393\n",
      "Epoch 111/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 6.0240\n",
      "Epoch 112/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 7.238 - 0s 40us/sample - loss: 5.9789\n",
      "Epoch 113/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 6.1868\n",
      "Epoch 114/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 4.918 - 0s 43us/sample - loss: 5.8646\n",
      "Epoch 115/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 5.9413\n",
      "Epoch 116/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 5.6910\n",
      "Epoch 117/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 5.7248\n",
      "Epoch 118/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 5.6267\n",
      "Epoch 119/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 5.5078\n",
      "Epoch 120/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 5.4343\n",
      "Epoch 121/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.510 - 0s 47us/sample - loss: 5.4484\n",
      "Epoch 122/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 5.2948\n",
      "Epoch 123/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 5.2607\n",
      "Epoch 124/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 5.2026\n",
      "Epoch 125/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 5.2249\n",
      "Epoch 126/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 5.1670\n",
      "Epoch 127/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 5.0662\n",
      "Epoch 128/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 5.0620\n",
      "Epoch 129/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 4.839 - 0s 55us/sample - loss: 5.0161\n",
      "Epoch 130/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 4.8870\n",
      "Epoch 131/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 4.163 - 0s 49us/sample - loss: 4.8783\n",
      "Epoch 132/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 5.1022\n",
      "Epoch 133/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.8157\n",
      "Epoch 134/500\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 4.8911\n",
      "Epoch 135/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.8318\n",
      "Epoch 136/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 4.6633\n",
      "Epoch 137/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 4.5911\n",
      "Epoch 138/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 4.6614\n",
      "Epoch 139/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 4.6473\n",
      "Epoch 140/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.5842\n",
      "Epoch 141/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 4.5499\n",
      "Epoch 142/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.4069\n",
      "Epoch 143/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 4.821 - 0s 63us/sample - loss: 4.3528\n",
      "Epoch 144/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 4.2919\n",
      "Epoch 145/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.3309\n",
      "Epoch 146/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 4.3432\n",
      "Epoch 147/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.2763\n",
      "Epoch 148/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.135 - 0s 45us/sample - loss: 4.1539\n",
      "Epoch 149/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 4.1593\n",
      "Epoch 150/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 4.0394\n",
      "Epoch 151/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 4.1335\n",
      "Epoch 152/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 4.0757\n",
      "Epoch 153/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 3.9110\n",
      "Epoch 154/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 4.0261\n",
      "Epoch 155/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 4.0181\n",
      "Epoch 156/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 4.3587\n",
      "Epoch 157/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 4.0651\n",
      "Epoch 158/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 4.0650\n",
      "Epoch 159/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 3.8800\n",
      "Epoch 160/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 3.8502\n",
      "Epoch 161/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 3.8086\n",
      "Epoch 162/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.791 - 0s 73us/sample - loss: 3.6835\n",
      "Epoch 163/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 3.7068\n",
      "Epoch 164/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 3.6578\n",
      "Epoch 165/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 3.6730\n",
      "Epoch 166/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 3.6027\n",
      "Epoch 167/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 3.6141\n",
      "Epoch 168/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 3.6286\n",
      "Epoch 169/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 3.4421\n",
      "Epoch 170/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 3.5604\n",
      "Epoch 171/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 3.4920\n",
      "Epoch 172/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 3.4242\n",
      "Epoch 173/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 3.4937\n",
      "Epoch 174/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 3.5438\n",
      "Epoch 175/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 3.5248\n",
      "Epoch 176/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 3.4716\n",
      "Epoch 177/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 4.121 - 0s 57us/sample - loss: 3.4515\n",
      "Epoch 178/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 3.3653\n",
      "Epoch 179/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 3.4810\n",
      "Epoch 180/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.106 - 0s 40us/sample - loss: 3.3300\n",
      "Epoch 181/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 3.2902\n",
      "Epoch 182/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 3.2752\n",
      "Epoch 183/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.933 - 0s 38us/sample - loss: 3.1994\n",
      "Epoch 184/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 34us/sample - loss: 3.2510\n",
      "Epoch 185/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 3.1782\n",
      "Epoch 186/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 3.1999\n",
      "Epoch 187/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 3.1482\n",
      "Epoch 188/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 3.1560\n",
      "Epoch 189/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 3.1672\n",
      "Epoch 190/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 3.1090\n",
      "Epoch 191/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.985 - 0s 51us/sample - loss: 3.0343\n",
      "Epoch 192/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.742 - 0s 30us/sample - loss: 3.0880\n",
      "Epoch 193/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 3.0279\n",
      "Epoch 194/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 3.1448\n",
      "Epoch 195/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 3.2781\n",
      "Epoch 196/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 2.9698\n",
      "Epoch 197/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.331 - 0s 47us/sample - loss: 3.0542\n",
      "Epoch 198/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.325 - 0s 51us/sample - loss: 2.9449\n",
      "Epoch 199/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 3.0467\n",
      "Epoch 200/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 2.9310\n",
      "Epoch 201/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.8917\n",
      "Epoch 202/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.768 - 0s 40us/sample - loss: 2.8732\n",
      "Epoch 203/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 2.8883\n",
      "Epoch 204/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 2.8441\n",
      "Epoch 205/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.224 - 0s 36us/sample - loss: 2.8597\n",
      "Epoch 206/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 3.0135\n",
      "Epoch 207/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 3.2086\n",
      "Epoch 208/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.9310\n",
      "Epoch 209/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 2.9092\n",
      "Epoch 210/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.573 - 0s 59us/sample - loss: 2.9149\n",
      "Epoch 211/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.9156\n",
      "Epoch 212/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 2.9814\n",
      "Epoch 213/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 2.8108\n",
      "Epoch 214/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.7414\n",
      "Epoch 215/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.8745\n",
      "Epoch 216/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 3.0826\n",
      "Epoch 217/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.9026\n",
      "Epoch 218/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.7228\n",
      "Epoch 219/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 2.6394\n",
      "Epoch 220/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.207 - 0s 53us/sample - loss: 2.6448\n",
      "Epoch 221/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 2.7335\n",
      "Epoch 222/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.6929\n",
      "Epoch 223/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.737 - 0s 40us/sample - loss: 2.7066\n",
      "Epoch 224/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 2.6746\n",
      "Epoch 225/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 2.6901\n",
      "Epoch 226/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 2.5711\n",
      "Epoch 227/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.6175\n",
      "Epoch 228/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 2.6220\n",
      "Epoch 229/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 2.6524\n",
      "Epoch 230/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 2.5814\n",
      "Epoch 231/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.944 - 0s 69us/sample - loss: 2.5972\n",
      "Epoch 232/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.344 - 0s 42us/sample - loss: 2.5840\n",
      "Epoch 233/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.5748\n",
      "Epoch 234/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 2.4931\n",
      "Epoch 235/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 2.4945\n",
      "Epoch 236/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 2.4774\n",
      "Epoch 237/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.4476\n",
      "Epoch 238/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 2.4749\n",
      "Epoch 239/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 2.5595\n",
      "Epoch 240/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.471 - 0s 49us/sample - loss: 2.4303\n",
      "Epoch 241/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 2.4293\n",
      "Epoch 242/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 2.4402\n",
      "Epoch 243/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 2.3760\n",
      "Epoch 244/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.671 - 0s 55us/sample - loss: 2.3748\n",
      "Epoch 245/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 2.3306\n",
      "Epoch 246/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 2.3573\n",
      "Epoch 247/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.160 - 0s 30us/sample - loss: 2.3345\n",
      "Epoch 248/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 2.4530\n",
      "Epoch 249/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 2.4208\n",
      "Epoch 250/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 2.3389\n",
      "Epoch 251/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 2.3708\n",
      "Epoch 252/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.3462\n",
      "Epoch 253/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 3.205 - 0s 53us/sample - loss: 2.3146\n",
      "Epoch 254/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.153 - 0s 57us/sample - loss: 2.3392\n",
      "Epoch 255/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 2.4183\n",
      "Epoch 256/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.2813\n",
      "Epoch 257/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 2.2856\n",
      "Epoch 258/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.3047\n",
      "Epoch 259/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 2.3495\n",
      "Epoch 260/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 2.3144\n",
      "Epoch 261/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 2.3377\n",
      "Epoch 262/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 2.2741\n",
      "Epoch 263/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 2.1920\n",
      "Epoch 264/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 2.2991\n",
      "Epoch 265/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.587 - 0s 36us/sample - loss: 2.2591\n",
      "Epoch 266/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 2.3057\n",
      "Epoch 267/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 2.2451\n",
      "Epoch 268/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 2.2999\n",
      "Epoch 269/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 2.1603\n",
      "Epoch 270/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 2.2138\n",
      "Epoch 271/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 2.1562\n",
      "Epoch 272/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 2.1944\n",
      "Epoch 273/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 38us/sample - loss: 2.1912\n",
      "Epoch 274/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.999 - 0s 71us/sample - loss: 2.1377\n",
      "Epoch 275/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.1375\n",
      "Epoch 276/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.327 - 0s 45us/sample - loss: 2.0985\n",
      "Epoch 277/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 2.1007\n",
      "Epoch 278/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.1282\n",
      "Epoch 279/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 2.0491\n",
      "Epoch 280/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 2.0286\n",
      "Epoch 281/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.0172\n",
      "Epoch 282/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.0446\n",
      "Epoch 283/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 2.0473\n",
      "Epoch 284/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.359 - 0s 47us/sample - loss: 2.0327\n",
      "Epoch 285/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 2.0369\n",
      "Epoch 286/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 2.3080\n",
      "Epoch 287/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 2.1531\n",
      "Epoch 288/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.1737\n",
      "Epoch 289/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 2.2151\n",
      "Epoch 290/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 2.1592\n",
      "Epoch 291/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 2.2577\n",
      "Epoch 292/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 2.5891\n",
      "Epoch 293/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 2.6105\n",
      "Epoch 294/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.328 - 0s 34us/sample - loss: 2.3421\n",
      "Epoch 295/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 2.3123\n",
      "Epoch 296/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.047 - 0s 40us/sample - loss: 2.2622\n",
      "Epoch 297/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.9875\n",
      "Epoch 298/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.9695\n",
      "Epoch 299/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.9117\n",
      "Epoch 300/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.9125\n",
      "Epoch 301/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.9044\n",
      "Epoch 302/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.318 - 0s 40us/sample - loss: 1.8811\n",
      "Epoch 303/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.8919\n",
      "Epoch 304/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.8983\n",
      "Epoch 305/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.621 - 0s 51us/sample - loss: 1.8759\n",
      "Epoch 306/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 1.9121\n",
      "Epoch 307/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.9004\n",
      "Epoch 308/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.9099\n",
      "Epoch 309/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.9566\n",
      "Epoch 310/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.9216\n",
      "Epoch 311/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 1.9246\n",
      "Epoch 312/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.9318\n",
      "Epoch 313/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.448 - 0s 57us/sample - loss: 1.9117\n",
      "Epoch 314/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 1.8245\n",
      "Epoch 315/500\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 1.8159\n",
      "Epoch 316/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.780 - 0s 59us/sample - loss: 1.9127\n",
      "Epoch 317/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.8593\n",
      "Epoch 318/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 1.9015\n",
      "Epoch 319/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.8563\n",
      "Epoch 320/500\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 1.8425\n",
      "Epoch 321/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.7661\n",
      "Epoch 322/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.087 - 0s 42us/sample - loss: 1.7939\n",
      "Epoch 323/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.7933\n",
      "Epoch 324/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.8175\n",
      "Epoch 325/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.7572\n",
      "Epoch 326/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.106 - 0s 49us/sample - loss: 1.9046\n",
      "Epoch 327/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 1.8064\n",
      "Epoch 328/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.7804\n",
      "Epoch 329/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 1.8331\n",
      "Epoch 330/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.7207\n",
      "Epoch 331/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 1.7349\n",
      "Epoch 332/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.7869\n",
      "Epoch 333/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.6981\n",
      "Epoch 334/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 1.7368\n",
      "Epoch 335/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 1.7315\n",
      "Epoch 336/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 1.6814\n",
      "Epoch 337/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.082 - 0s 73us/sample - loss: 1.6829\n",
      "Epoch 338/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 1.7465\n",
      "Epoch 339/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 1.6517\n",
      "Epoch 340/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.6646\n",
      "Epoch 341/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 1.6549\n",
      "Epoch 342/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.6232\n",
      "Epoch 343/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.6177\n",
      "Epoch 344/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.6036\n",
      "Epoch 345/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.6999\n",
      "Epoch 346/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.6405\n",
      "Epoch 347/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 1.6328\n",
      "Epoch 348/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.6083\n",
      "Epoch 349/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.6660\n",
      "Epoch 350/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.7125\n",
      "Epoch 351/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.7637\n",
      "Epoch 352/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.8014\n",
      "Epoch 353/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.781 - 0s 55us/sample - loss: 1.6909\n",
      "Epoch 354/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.6954\n",
      "Epoch 355/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.6785\n",
      "Epoch 356/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.7235\n",
      "Epoch 357/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.6997\n",
      "Epoch 358/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 1.6798\n",
      "Epoch 359/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 1.7144\n",
      "Epoch 360/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.6367\n",
      "Epoch 361/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.5490\n",
      "Epoch 362/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.5499\n",
      "Epoch 363/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 42us/sample - loss: 1.5197\n",
      "Epoch 364/500\n",
      "506/506 [==============================] - 0s 99us/sample - loss: 1.5652\n",
      "Epoch 365/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.896 - 0s 67us/sample - loss: 1.5127\n",
      "Epoch 366/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.5660\n",
      "Epoch 367/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.5435\n",
      "Epoch 368/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.5162\n",
      "Epoch 369/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.5036\n",
      "Epoch 370/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 1.5489\n",
      "Epoch 371/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.5272\n",
      "Epoch 372/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.5959\n",
      "Epoch 373/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.5571\n",
      "Epoch 374/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 1.5394\n",
      "Epoch 375/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.5756\n",
      "Epoch 376/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.5379\n",
      "Epoch 377/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.4823\n",
      "Epoch 378/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.875 - 0s 38us/sample - loss: 1.5021\n",
      "Epoch 379/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 1.4745\n",
      "Epoch 380/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.4355\n",
      "Epoch 381/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.4735\n",
      "Epoch 382/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.4689\n",
      "Epoch 383/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.4966\n",
      "Epoch 384/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.482 - 0s 65us/sample - loss: 1.5040\n",
      "Epoch 385/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.237 - 0s 45us/sample - loss: 1.6061\n",
      "Epoch 386/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.4764\n",
      "Epoch 387/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.5155\n",
      "Epoch 388/500\n",
      "506/506 [==============================] - 0s 36us/sample - loss: 1.5234\n",
      "Epoch 389/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.6557\n",
      "Epoch 390/500\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 1.5088\n",
      "Epoch 391/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.4716\n",
      "Epoch 392/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 1.4790\n",
      "Epoch 393/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.4878\n",
      "Epoch 394/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 1.3981\n",
      "Epoch 395/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.898 - 0s 59us/sample - loss: 1.4738\n",
      "Epoch 396/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.760 - 0s 45us/sample - loss: 1.4640\n",
      "Epoch 397/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.3678\n",
      "Epoch 398/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.5279\n",
      "Epoch 399/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 1.5061\n",
      "Epoch 400/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.4430\n",
      "Epoch 401/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.4259\n",
      "Epoch 402/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.3992\n",
      "Epoch 403/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.3767\n",
      "Epoch 404/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.4452\n",
      "Epoch 405/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.5015\n",
      "Epoch 406/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.3681\n",
      "Epoch 407/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.4228\n",
      "Epoch 408/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.3319\n",
      "Epoch 409/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.3612\n",
      "Epoch 410/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.3253\n",
      "Epoch 411/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.3289\n",
      "Epoch 412/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 1.3365\n",
      "Epoch 413/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 1.3196\n",
      "Epoch 414/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.4053\n",
      "Epoch 415/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.3395\n",
      "Epoch 416/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.4670\n",
      "Epoch 417/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 1.5368\n",
      "Epoch 418/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.544 - 0s 34us/sample - loss: 1.4352\n",
      "Epoch 419/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.3848\n",
      "Epoch 420/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.3980\n",
      "Epoch 421/500\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 1.4063\n",
      "Epoch 422/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 1.2653\n",
      "Epoch 423/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 1.3710\n",
      "Epoch 424/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.2543\n",
      "Epoch 425/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.054 - 0s 75us/sample - loss: 1.3464\n",
      "Epoch 426/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.3224\n",
      "Epoch 427/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.3103\n",
      "Epoch 428/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.2353\n",
      "Epoch 429/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.2568\n",
      "Epoch 430/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.2098\n",
      "Epoch 431/500\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 1.2697\n",
      "Epoch 432/500\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 1.2269\n",
      "Epoch 433/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 1.2535\n",
      "Epoch 434/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 1.2700\n",
      "Epoch 435/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.2160\n",
      "Epoch 436/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.2561\n",
      "Epoch 437/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 1.2116\n",
      "Epoch 438/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.2002\n",
      "Epoch 439/500\n",
      "506/506 [==============================] - 0s 59us/sample - loss: 1.1988\n",
      "Epoch 440/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.2102\n",
      "Epoch 441/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.2054\n",
      "Epoch 442/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.1851\n",
      "Epoch 443/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.1962\n",
      "Epoch 444/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.2082\n",
      "Epoch 445/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.2421\n",
      "Epoch 446/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.2681\n",
      "Epoch 447/500\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 1.3158\n",
      "Epoch 448/500\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 1.2578\n",
      "Epoch 449/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.2241\n",
      "Epoch 450/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.2498\n",
      "Epoch 451/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.212 - 0s 61us/sample - loss: 1.2662\n",
      "Epoch 452/500\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 1.4504\n",
      "Epoch 453/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.500 - 0s 67us/sample - loss: 1.2579\n",
      "Epoch 454/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 30us/sample - loss: 1.1955\n",
      "Epoch 455/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.1625\n",
      "Epoch 456/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.1697\n",
      "Epoch 457/500\n",
      "506/506 [==============================] - 0s 55us/sample - loss: 1.1380\n",
      "Epoch 458/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.1541\n",
      "Epoch 459/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.2673\n",
      "Epoch 460/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.086 - 0s 53us/sample - loss: 1.2671\n",
      "Epoch 461/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 1.2216\n",
      "Epoch 462/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.974 - 0s 59us/sample - loss: 1.1954\n",
      "Epoch 463/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.1612\n",
      "Epoch 464/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.681 - 0s 42us/sample - loss: 1.1528\n",
      "Epoch 465/500\n",
      "506/506 [==============================] - 0s 57us/sample - loss: 1.1248\n",
      "Epoch 466/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.1216\n",
      "Epoch 467/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.353 - 0s 51us/sample - loss: 1.1260\n",
      "Epoch 468/500\n",
      "506/506 [==============================] - 0s 30us/sample - loss: 1.1051\n",
      "Epoch 469/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.1091\n",
      "Epoch 470/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.1095\n",
      "Epoch 471/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.0851\n",
      "Epoch 472/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.095 - 0s 43us/sample - loss: 1.1219\n",
      "Epoch 473/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.1412\n",
      "Epoch 474/500\n",
      "506/506 [==============================] - 0s 53us/sample - loss: 1.1277\n",
      "Epoch 475/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.1059\n",
      "Epoch 476/500\n",
      "506/506 [==============================] - 0s 34us/sample - loss: 1.1119\n",
      "Epoch 477/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.0878\n",
      "Epoch 478/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.835 - 0s 38us/sample - loss: 1.0960\n",
      "Epoch 479/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.0820\n",
      "Epoch 480/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.0887\n",
      "Epoch 481/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.1046\n",
      "Epoch 482/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.0918\n",
      "Epoch 483/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.0587\n",
      "Epoch 484/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.942 - 0s 49us/sample - loss: 1.0723\n",
      "Epoch 485/500\n",
      "506/506 [==============================] - 0s 32us/sample - loss: 1.0747\n",
      "Epoch 486/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.0968\n",
      "Epoch 487/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.0350\n",
      "Epoch 488/500\n",
      "506/506 [==============================] - 0s 40us/sample - loss: 1.0720\n",
      "Epoch 489/500\n",
      "506/506 [==============================] - 0s 42us/sample - loss: 1.0315\n",
      "Epoch 490/500\n",
      "506/506 [==============================] - 0s 51us/sample - loss: 1.0262\n",
      "Epoch 491/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.0599\n",
      "Epoch 492/500\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 1.0623\n",
      "Epoch 493/500\n",
      "506/506 [==============================] - 0s 49us/sample - loss: 1.0604\n",
      "Epoch 494/500\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.328 - 0s 57us/sample - loss: 1.1311\n",
      "Epoch 495/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.2293\n",
      "Epoch 496/500\n",
      "506/506 [==============================] - 0s 43us/sample - loss: 1.1693\n",
      "Epoch 497/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.3027\n",
      "Epoch 498/500\n",
      "506/506 [==============================] - 0s 47us/sample - loss: 1.0932\n",
      "Epoch 499/500\n",
      "506/506 [==============================] - 0s 38us/sample - loss: 1.0736\n",
      "Epoch 500/500\n",
      "506/506 [==============================] - 0s 45us/sample - loss: 1.1525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x277a42b0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=500, batch_size=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大致一样的参数和数据在keras框架下训练得到loss和之前写的框架基本一致\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考\n",
    "1. 以上从0实现的框架对于weight 和bias 的处理其实也把其看成了一个神经单元，而weight bias的数值存在该单元属性内。\n",
    "2. 如果不把weight 和bias看作一个神经单元，而是把其数值存入到其所属layer的属性里面(相当于weight和bias的值是神经层的一个属性)，以此来建立的框架又和本次实现的框架有什么不同？其各自的优缺点又什么什么？\n",
    "3. 可以把sigmoid函数在节点计算value的时候就植入，提供激活函数选择，或不选择，根据用户输入来决定。这样做就避免我们实际构建关系时在中间添加一个sigmoid节点层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
