{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 电影评论分类\n",
    "\n",
    "1. 数据预处理\n",
    "2. 句子向量化\n",
    "3. 使用神经网络对其进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 数据提取以及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = 'E:/MYGIT/DataSources/movie_comments.csv'\n",
    "comments_pd = pd.read_csv(data_path,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261497, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>犯我中华者虽远必诛，是有多无脑才信这句话。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>这部戏让人看的热血沸腾，对吴京路转粉，最后的彩蛋，让我们没有理由不期待下一部。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>假嗨，特别恶心的电影。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>有几处情节设置过于尴尬，彰显国家自豪感的部分稍显突兀。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>就是一部爽片，打戏挺燃，但是故事一般。达康书记不合适这个角色，赵东来倒是很合适。张瀚太太太违...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>赵东来：达康书记，我们接到在非洲卧底的冷锋报告，丁义珍现在非洲，我们请求抓捕。李达康：东来，...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>下一部拍喜剧吧，整个片子真感觉挺搞笑的</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>《战狼2》里吴京这么能打，他打得过徐晓冬么？</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>心往一处想，劲往一处使，就能实现我们的梦想。看吧，比第一部好太多了。谢谢美队的动作指导。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>这都能火。是我没见识！</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>开头的水下长对决戏可算华语电影的顶尖存在；驱逐舰、导弹和坦克在商业片里这么狂用也是了得；镜头...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>很用心啊吴京导演，小看你了，确实在导演上下功夫了拉片子了，知道借鉴是好的。至于大家比较反感的...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>犯我中华者虽远必诛，这句话一直在我脑子里回响</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>片头海里那场动作戏看完就呆不下去了，太假太做作，提前离场。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>好看，这部戏让人看的热血沸腾，打戏挺燃的，吴京演技棒呆了</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>符合“有钱了续集反而拍更差”这一放之四海而皆准的规律，场面越做越大，然而伴随着各种动作场面和...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>说喜欢这部片子的人不是装傻就是真傻，要不是真的没有别的可看肯定是不会选这部的，直男癌到令人发...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>整部电影延续1的风格，热血。场面比1来的要大，打戏动作不错，吴京挺适合演军人的，电影之前的中...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>目瞪狗呆！太瘠薄好看了！中国人牛b就是硬道理！隔壁建军大爷都没你们爱国</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>《战狼2》的动作场景和战斗装备全线升级，热血的打斗动作从头打到尾。《战狼2》游走在电影审查红...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261467</th>\n",
       "      <td>260120</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>bg戏略出戏 结尾坑爹 不过男主真的有一双漂亮可爱的大眼睛啊太萌 但是我觉得他为人略让人受不了</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261468</th>\n",
       "      <td>260121</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>还行。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261469</th>\n",
       "      <td>260122</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>无聊= =拍摄手法GJ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261470</th>\n",
       "      <td>260123</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>小男主游泳游得真心挺快~</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261471</th>\n",
       "      <td>260124</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>Après avoir vu ce film à 2 heures 10, moi, je ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261472</th>\n",
       "      <td>260125</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>第一個吻好美。不卑不亢。整體感覺良好。這是比較現實和年輕的故事。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261473</th>\n",
       "      <td>260126</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>文森忧郁的眼神叫人心疼。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261474</th>\n",
       "      <td>260127</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>还不错，最后结尾互相追逐的那个恶寒啊...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261475</th>\n",
       "      <td>260128</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>百度云居然。。。把我存的这片子禁了？！哈？！</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261476</th>\n",
       "      <td>260129</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>话说法国出帅哥，法国人喜欢浪漫，冲着片名我点犹豫都没有的看了，少年美不美真不好说，但是周围人...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261477</th>\n",
       "      <td>260130</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>第一個吻好美。不卑不亢。整體感覺良好。這是比較現實和年輕的故事。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261478</th>\n",
       "      <td>260131</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>文森忧郁的眼神叫人心疼。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261479</th>\n",
       "      <td>260132</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>还不错，最后结尾互相追逐的那个恶寒啊...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261480</th>\n",
       "      <td>260133</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>百度云居然。。。把我存的这片子禁了？！哈？！</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261481</th>\n",
       "      <td>260134</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>话说法国出帅哥，法国人喜欢浪漫，冲着片名我点犹豫都没有的看了，少年美不美真不好说，但是周围人...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261482</th>\n",
       "      <td>260135</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>生活、爱情、亲情、友情、选择。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261483</th>\n",
       "      <td>260136</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>一般般</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261484</th>\n",
       "      <td>260137</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>电影一般,可以随便看看.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261485</th>\n",
       "      <td>260138</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>我到底是怎么看完它的..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261486</th>\n",
       "      <td>260139</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>成长的烦恼，美型的少年，难以看清的真相。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261487</th>\n",
       "      <td>260140</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>轻松喜剧电影</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261488</th>\n",
       "      <td>260141</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>温暖的剧情,温暖的结局...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261489</th>\n",
       "      <td>260142</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>主角们都很好看 又是一个碧悲情的小女主角\\r\\n又是一段关于十七岁的故事</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261490</th>\n",
       "      <td>260143</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>一切都是爲了顏！！！= =||||||</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261491</th>\n",
       "      <td>260144</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>大概看了\\r\\n总的来说还是不错 其实内心是十分期待男主和他的好友发生点什么的 哎。。。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261492</th>\n",
       "      <td>260145</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>内容只能说一般。。女性角色怎么都这么悲催啊！！不过男猪脚很帅</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261493</th>\n",
       "      <td>260146</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>翘了三天班就窝在家里看小基片惹（手动拜拜.gif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261494</th>\n",
       "      <td>260147</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261495</th>\n",
       "      <td>260148</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>冲着颜值还可以看下去</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261496</th>\n",
       "      <td>260149</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261497 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                        link  \\\n",
       "0            1  https://movie.douban.com/subject/26363254/   \n",
       "1            2  https://movie.douban.com/subject/26363254/   \n",
       "2            3  https://movie.douban.com/subject/26363254/   \n",
       "3            4  https://movie.douban.com/subject/26363254/   \n",
       "4            5  https://movie.douban.com/subject/26363254/   \n",
       "...        ...                                         ...   \n",
       "261492  260145   https://movie.douban.com/subject/1441763/   \n",
       "261493  260146   https://movie.douban.com/subject/1441763/   \n",
       "261494  260147   https://movie.douban.com/subject/1441763/   \n",
       "261495  260148   https://movie.douban.com/subject/1441763/   \n",
       "261496  260149   https://movie.douban.com/subject/1441763/   \n",
       "\n",
       "                             name  \\\n",
       "0                             战狼2   \n",
       "1                             战狼2   \n",
       "2                             战狼2   \n",
       "3                             战狼2   \n",
       "4                             战狼2   \n",
       "...                           ...   \n",
       "261492  不羁美少年 À cause d'un garçon   \n",
       "261493  不羁美少年 À cause d'un garçon   \n",
       "261494  不羁美少年 À cause d'un garçon   \n",
       "261495  不羁美少年 À cause d'un garçon   \n",
       "261496  不羁美少年 À cause d'un garçon   \n",
       "\n",
       "                                                  comment star  \n",
       "0                                      吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1       首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2       吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                           凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                                    中二得很    1  \n",
       "...                                                   ...  ...  \n",
       "261492                     内容只能说一般。。女性角色怎么都这么悲催啊！！不过男猪脚很帅    3  \n",
       "261493                           翘了三天班就窝在家里看小基片惹（手动拜拜.gif    3  \n",
       "261494      我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）    2  \n",
       "261495                                         冲着颜值还可以看下去    3  \n",
       "261496                 除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心    3  \n",
       "\n",
       "[261497 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "moive_stars = list(comments_pd['star'])\n",
    "movie_comments = list(comments_pd['comment'])\n",
    "#movie_name = [str(name) for name in movie_name]\n",
    "#movie_comments = [str(comm) for comm in movie_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261497 261497\n"
     ]
    }
   ],
   "source": [
    "print(len(moive_stars), len(movie_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 好看，这部戏让人看的热血沸腾，打戏挺燃的，吴京演技棒呆了\n"
     ]
    }
   ],
   "source": [
    "print(moive_stars[24],movie_comments[24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "文本处理，剔除一些干扰评论，剔除是英文的评论\n",
    "\n",
    "思考？应不应该剔除那些评论字数少于某特定值的数据\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "第一次清洗，去除评论为空,或评论字数只有一两个词的评论\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13100 1\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "removes1 = [_i for _i,comm in enumerate(movie_comments) if (not isinstance(comm, str)) \n",
    "           or (comm == '')  or (len(comm) < 4)]\n",
    "removes2 = [_i for _i,comm in enumerate(moive_stars) if comm not in ['1','2','3','4','5']]\n",
    "print(len(removes1), len(removes2))\n",
    "removes = removes1+removes2\n",
    "removes = set(removes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "moive_stars = [star for _i,star in enumerate(moive_stars) if _i not in removes]\n",
    "movie_comments = [comm for _i,comm in enumerate(movie_comments) if _i not in removes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#繁体字转简体字\n",
    "from langconv import *\n",
    "def cht_to_chs(line):\n",
    "    line = Converter('zh-hans').convert(line)\n",
    "    line.encode('utf-8')\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248396 248396\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_comments),len(moive_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#繁体字转简体处理\n",
    "movie_comments = [cht_to_chs(comm) for comm in movie_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248396 248396\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_comments),len(moive_stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "第二次清洗，剔除那些评论不是中文的评论数据\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ja', -56.275206565856934)\n"
     ]
    }
   ],
   "source": [
    "string1 = '爱和坚持。'\n",
    "print(langid.classify(string1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#舍弃不用该方法，效率太低了\n",
    "def is_en(string):\n",
    "    result = langid.classify(string)\n",
    "    if result[0] not in ('zh', 'ja'):\n",
    "    #if result[0] != 'zh':\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def not_ch(string):\n",
    "    length = len(string)\n",
    "    length_ch = 0\n",
    "    for ch in string:\n",
    "        if '\\u4e00'  <=  ch <='\\u9fff':\n",
    "            length_ch +=1\n",
    "    if length_ch < length*0.35:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You got a dream, you gotta protect it. Don't let your dreams be dreams.\n",
      "You have a dream, you got to protect it.\n",
      "好！好！！好！！！\n",
      "好！好！！好！！！\n",
      "MIT？\n",
      "This is part of my life……\n",
      "MIT？\n",
      "This is part of my life……\n",
      "http://www.douban.com/review/1279547/\n",
      "2008.2.3\n",
      "Smith毁容鸟...\n",
      "2008.2.3\n",
      "Smith毁容鸟...\n",
      "http://v.youku.com/v_show/id_XODY3ODI3Ng==.html\n",
      "真尴尬啊。大结局居然是十年后问了声“my name is hanmeimei whats your name？”\n",
      "真尴尬啊。大结局居然是十年后问了声“my name is hanmeimei whats your name？”\n",
      "真尴尬啊。大结局居然是十年后问了声“my name is hanmeimei whats your name？”\n",
      "我只认识sandy and sue，再见\n",
      "ヽ(´o｀；\n",
      "ヽ(´o｀；\n",
      "ヽ(´o｀；\n",
      "ヽ(´o｀；\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1000):\n",
    "    if not_ch(movie_comments[i]):\n",
    "        print(movie_comments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248396 248396\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_comments),len(moive_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10691\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "removes = [_i for _i,comm in enumerate(movie_comments) if not_ch(comm)]\n",
    "removes = set(removes)\n",
    "print(len(removes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stars = [star for _i,star in enumerate(moive_stars) if _i not in removes]\n",
    "comments = [comm for _i,comm in enumerate(movie_comments) if _i not in removes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237705 237705\n"
     ]
    }
   ],
   "source": [
    "print(len(comments),len(stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 22176, '2': 25980, '4': 76397, '5': 52932, '3': 60220})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "starsdict = Counter(stars)\n",
    "starsdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#获取每个类别的评论\n",
    "from collections import defaultdict\n",
    "def get_each_classdata(dataset, labels):\n",
    "    datas = defaultdict(list)\n",
    "    for i in range(len(labels)):\n",
    "        datas[labels[i]].append(dataset[i])\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label_data_dict = get_each_classdata(comments, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1', '2', '4', '5', '3'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "如果直接是按5个星级来分类，由于评论对应星级评判太感性，太情感化，即使是人都不能准确地把某一评论具体定位到1分，2分，3分等量化分数上，更别提机器了，这里为了使得实际效果好些，改成两分类问题，把1星2星的评论归为差评， 4星5星评论归为好评。 3星评论暂时舍弃不用\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dict = defaultdict(list)\n",
    "data_dict[0] = label_data_dict['1']+ label_data_dict['2']\n",
    "data_dict[1] = label_data_dict['4']+ label_data_dict['5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48156\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#均衡下正负样本\n",
    "num_per_class = 45000\n",
    "def get_balanced_dataset(datas, num_per_class):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for key in datas:\n",
    "        if len(datas[key]) < num_per_class:\n",
    "            print('testpoint')\n",
    "            dataset += datas[key]\n",
    "            labels += [key]*len(datas[key])\n",
    "        else:\n",
    "            dataset += random.sample(datas[key],num_per_class)\n",
    "            labels += [key]* num_per_class\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset,labels = get_balanced_dataset(data_dict, num_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels[45000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000 90000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神剧！我真的不知道该从哪里吐槽，槽点太多，剧情很神，不知道在讲什么，莫名其妙，断断续续没有主题或者说不知道想表达什么，动作戏很假，完全接不上，画面也是断断续续的，配音很不自然，演技浮夸，很尴尬…… 0\n"
     ]
    }
   ],
   "source": [
    "print(dataset[222],labels[222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法1：get sentence embedding from word embedding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sentence_embedding.png](https://i.loli.net/2019/09/10/InrCOWD1XxQSvld.png)\n",
    "\n",
    "[句子向量化论文参考](https://openreview.net/pdf?id=SyK00v5xx)\n",
    "- line2：计算每条句子的向量，以列存储 原文是 compute the weighted average of the word vector\n",
    "- line6：原文是 remove the projections of the average vectors on their first singular verctor(common component removal),至于为什么line6这样计算会是common component removal，还没弄明白，有空再深究\n",
    "- 最后得到的矩阵是M×N的 M为词向量维度，N为句子个数，可以看出每个句子向量都是以列保存的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "code_folding": [
     30
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "\n",
    "#该函数对应的是上诉图片中伪代码的实现过程\n",
    "def get_sentences_vec(model_wv, sent_list, word_frequence):\n",
    "    # 句子向量化处理\n",
    "    a = 0.001\n",
    "    row = model_wv.vector_size\n",
    "    col = len(sent_list)\n",
    "    sent_mat = np.zeros((row, col))\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        length = len(sent)\n",
    "        sent_vec = np.zeros(row)\n",
    "        for word in sent:\n",
    "            pw = word_frequence[word]\n",
    "            w = a / (a + pw)\n",
    "            try:\n",
    "                vec = np.array(model_wv[word])\n",
    "                sent_vec += w * vec\n",
    "            except:\n",
    "                pass\n",
    "        sent_mat[:, i] += sent_vec\n",
    "        sent_mat[:, i] /= length\n",
    "\n",
    "    # PCA处理\n",
    "    sent_mat = np.mat(sent_mat)\n",
    "    u, s, vh = np.linalg.svd(sent_mat, full_matrices=False)\n",
    "    sent_mat = sent_mat - u * u.T * sent_mat\n",
    "    return sent_mat\n",
    "\n",
    "def get_word_frequence(words):\n",
    "    #这里不做停用次处理，直接在计算句子向量时候，如果找不到该词，直接跳过\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        word_list += word\n",
    "    word_frequence = Counter(word_list)\n",
    "    return word_frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('stopwords.txt') as f:\n",
    "    line = f.readline()\n",
    "    while line !='':\n",
    "        stopwords.append(line.strip('\\n'))\n",
    "        line = f.readline()\n",
    "stopwords.append('\\n')\n",
    "stopwords.append(' ')\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "removes = []\n",
    "for _i,sen in enumerate(dataset):\n",
    "    sen = sen.strip('\\n')\n",
    "    words = jieba.lcut(sen)\n",
    "    temp = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            temp.append(word)\n",
    "    if temp == []:\n",
    "        removes.append(_i)\n",
    "    else:\n",
    "        corpus.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['看到', '名字', '想', '一星', '没', '看好']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 89869)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(removes),len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [lab for _i,lab in enumerate(labels) if _i not in removes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89869, 89869)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "fname = r\"E:/MYGIT/model/wiki_stopwords/wiki_word2vec.kv\"\n",
    "model_wv = KeyedVectors.load(fname, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequence = get_word_frequence(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comment_vecs = get_sentences_vec(model_wv, corpus, words_frequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_vecs = comment_vecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89869, 500), dtype('float64'))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_vecs.shape , comment_vecs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打乱数据\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])#\n",
    "    shuffled_dataset = dataset[permutation,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = randomize(comment_vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89869, 500), (89869,))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "这里对数据进行标准化处理,并把结果保存\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (dataset - np.mean(dataset, axis=0)) / np.std(dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集，验证集，测试集\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset,train_labels,test_labels = train_test_split(X_, labels, test_size=0.2)\n",
    "valid_dataset,test_dataset,valid_labels,test_labels = train_test_split(test_dataset, test_labels,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71895, 500) (8987, 500) (8987, 500)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape,valid_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#保存划分好的数据\n",
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'E:/MYGIT/DataSources/MovieComments/Comments_04'\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  方法2 : TF-IDF\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('stopwords.txt') as f:\n",
    "    line = f.readline()\n",
    "    while line !='':\n",
    "        stopwords.append(line.strip('\\n'))\n",
    "        line = f.readline()\n",
    "stopwords.append('\\n')\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('本片看得那个囧啊，每个人出场都那么囧。周杰伦为什么不用双截棍啊？', 0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[71], labels[71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Oliver\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.905 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sen in dataset:\n",
    "    words = jieba.lcut(sen)\n",
    "    temp = ''\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            temp += word + ' '\n",
    "    corpus.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'麻烦 胡歌 粉装路 装 一点 '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer(max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer1.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 2000)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "#a =(a - np.mean(a, axis=0)) / np.std(a, axis=0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset,test_dataset,train_labels,test_labels = train_test_split(X, labels, test_size=0.2)\n",
    "valid_dataset,test_dataset,valid_labels,test_labels = train_test_split(test_dataset, test_labels,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 2000) (9000, 2000) (9000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape,valid_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_dataset, train_labels)\n",
    "y_hat = clf.predict(valid_dataset)\n",
    "acc = format(accuracy_score(valid_labels, y_hat),'.4f')\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存划分好的数据\n",
    "#注意这里保存的TFIDF向量是用稀疏矩阵保存的\n",
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'E:/MYGIT/DataSources/MovieComments/Comments_TFIDF_02'\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (71895, 500) (71895,)\n",
      "Validation set (8987, 500) (8987,)\n",
      "Test set (8987, 500) (8987,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pickle_file = 'E:/MYGIT/DataSources/MovieComments/Comments_04'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del(save)  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (71895, 500) (71895,)\n",
      "Validation set (8987, 500) (8987,)\n",
      "Test set (8987, 500) (8987,)\n"
     ]
    }
   ],
   "source": [
    "#数据类型统一化，以及label变成one-hot编码\n",
    "num_labels = 2\n",
    "def reformat(datasets, labels):\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    datasets = datasets.astype(np.float32)\n",
    "    return datasets,labels\n",
    "\n",
    "train_dataset,train_labels1 = reformat(train_dataset,train_labels)\n",
    "valid_dataset,valid_labels1 = reformat(valid_dataset,valid_labels)\n",
    "test_dataset, test_labels1 = reformat(test_dataset,test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##先用简单模型测试一下，可以预判下之前的数据处理是否合理\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10,criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_hat = clf.predict(test_dataset)\n",
    "acc = format(accuracy_score(test_labels, y_hat),'.4f')\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.exponential_decay??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2层神经网络\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 128\n",
    "beta_regul = 1e-3\n",
    "feature_size = 500\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, feature_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    #第一层输入节点是28*28级原始数据的维度大小，本层节点是个数是num_hidden_nodes1\n",
    "    weights1 = tf.Variable(tf.truncated_normal([feature_size, num_hidden_nodes1],\n",
    "                                               stddev=np.sqrt(2.0 / (feature_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "    #第二层的输入节点是第一层节点个数，本层节点个数是num_hidden_nodes2,stddev是指定生成数据的标准差\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    #最后一层的输入节点个数是第二层节点个数，本层节点个数是要分类的类别个数。\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 1.619719\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 49.0%\n",
      "Minibatch loss at step 200: 1.497957\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 57.6%\n",
      "Minibatch loss at step 400: 1.463217\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 63.0%\n",
      "Minibatch loss at step 600: 1.433888\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 63.3%\n",
      "Minibatch loss at step 800: 1.380140\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 60.1%\n",
      "Minibatch loss at step 1000: 1.333677\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 62.9%\n",
      "Minibatch loss at step 1200: 1.309573\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 64.2%\n",
      "Minibatch loss at step 1400: 1.314891\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 64.4%\n",
      "Minibatch loss at step 1600: 1.238327\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 1800: 1.259339\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 64.2%\n",
      "Minibatch loss at step 2000: 1.266651\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 2200: 1.219433\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 2400: 1.158890\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 2600: 1.156308\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 2800: 1.149660\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 3000: 1.107849\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 3200: 1.216748\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 3400: 1.102898\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 3600: 1.112286\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 67.0%\n",
      "Test accuracy: 66.5%\n"
     ]
    }
   ],
   "source": [
    "#128*18001 = 2304128\n",
    "num_steps = 3601\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size)]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if(step % 200 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels1))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (72000, 2000) (72000,)\n",
      "Validation set (9000, 2000) (9000,)\n",
      "Test set (9000, 2000) (9000,)\n"
     ]
    }
   ],
   "source": [
    "#使用TFIDF得到句子向量\n",
    "import os\n",
    "pickle_file = 'E:/MYGIT/DataSources/MovieComments/Comments_TFIDF_02'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del(save)  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (72000, 2000) (72000,)\n",
      "Validation set (9000, 2000) (9000,)\n",
      "Test set (9000, 2000) (9000,)\n"
     ]
    }
   ],
   "source": [
    "#数据类型统一化，以及label变成one-hot编码\n",
    "num_labels = 2\n",
    "def reformat(datasets, labels):\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    datasets = datasets.astype(np.float32)\n",
    "    datasets = datasets.toarray()\n",
    "    return datasets,labels\n",
    "\n",
    "train_dataset,train_labels1 = reformat(train_dataset,train_labels)\n",
    "valid_dataset,valid_labels1 = reformat(valid_dataset,valid_labels)\n",
    "test_dataset, test_labels1 = reformat(test_dataset,test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2层神经网络\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 128\n",
    "beta_regul = 1e-3\n",
    "feature_size = 2000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, feature_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    #第一层输入节点是28*28级原始数据的维度大小，本层节点是个数是num_hidden_nodes1\n",
    "    weights1 = tf.Variable(tf.truncated_normal([feature_size, num_hidden_nodes1],\n",
    "                                               stddev=np.sqrt(2.0 / (feature_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "    #第二层的输入节点是第一层节点个数，本层节点个数是num_hidden_nodes2,stddev是指定生成数据的标准差\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    #最后一层的输入节点个数是第二层节点个数，本层节点个数是要分类的类别个数。\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1.586879\n",
      "Minibatch accuracy: 41.4%\n",
      "Validation accuracy: 50.7%\n",
      "Minibatch loss at step 200: 1.543147\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 61.7%\n",
      "Minibatch loss at step 400: 1.500564\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 600: 1.392491\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 800: 1.350853\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 1000: 1.311717\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 1200: 1.213789\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 1400: 1.230655\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 1600: 1.242313\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 1800: 1.115149\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 2000: 1.137029\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 2200: 1.046123\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 2400: 1.131557\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 2600: 1.107512\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 2800: 1.077618\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 3000: 1.036937\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3200: 1.143580\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3400: 1.067934\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 3600: 1.072373\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.5%\n",
      "Test accuracy: 76.0%\n"
     ]
    }
   ],
   "source": [
    "#128*3601 = 2304128\n",
    "num_steps = 3601\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size)]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if(step % 200 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels1))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果分析\n",
    "\n",
    "1. 方法1句子向量化得到的模型测试集准确率大概0.66左右，TF-IDF的到的准确率大概在0.76左右\n",
    "\n",
    "2. 为什么准确率都不是太理想？？\n",
    " - 原始评论数据本来区分就不是很明显，特别是比较中性的评论对结果影响较大，很难区分该评论是好是坏\n",
    " - 原始评论数据中一些一些评论看起来是正向评论，但标签却是负\n",
    " - 句子向量化的处理还不够好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
