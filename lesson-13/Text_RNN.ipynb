{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'E:/MYGIT/model/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('E:/MYGIT/DataSources/jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "test = pd.read_csv('E:/MYGIT/DataSources/jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "submission = pd.read_csv('E:/MYGIT/DataSources/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_csv('E:/MYGIT/DataSources/jigsaw-toxic-comment-classification-challenge/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "X_train_ = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 153164)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#限制下训练数据大小，本机器配置不行\n",
    "X_train = X_train[:10000]\n",
    "X_test = X_test[:10000]\n",
    "y_train = y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 40000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "##把corpus序列化，保存前100000个词作为字典,会分词过滤标点等，只适用于英文\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#padding使得所有序列一样长,不够的往前填充0，多的保留后200个\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 159571)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './np'\n",
    "pickle_file = os.path.join(data_root, 'toxic_classification.pickle')\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        save = {\n",
    "        'x_train': x_train,\n",
    "        'y_train': y_train,\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test,\n",
    "        'embedding_matrix': embedding_matrix,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "#读取fasttext词向量\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从fasttext词向量获取训练数据中tokens的所有词向量\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del embeddings_index\n",
    "#gc回收\n",
    "import gc\n",
    "unreachable_count = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./np/embedding_matrix_rnn', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 100000\n",
    "# maxlen = 200\n",
    "# embed_size = 300\n",
    "\n",
    "filter_sizes = [1,2,3,5]\n",
    "HIDDNE_SIZE_1 = 256\n",
    "HIDDNE_SIZE_2 = 128\n",
    "\n",
    "def get_model():\n",
    "    ###embedding 和textcnn一样处理\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    #conv_0 = Conv1D(num_filters, kernel_size=kernel_size, strides=1)(x_emb)#这里也可以用conv1D，因为在embed_size\n",
    "    #等于词向量维度大小，故在列方向相当于没有做卷积操作，使用Conv2D的效果和Conv1D一样\n",
    "    rnn1 = SimpleRNN(HIDDNE_SIZE_1, return_sequences=True, recurrent_dropout= 0.2,input_shape=(maxlen, embed_size))(x)\n",
    "    \n",
    "    rnn2 = SimpleRNN(HIDDNE_SIZE_1, return_sequences=False, recurrent_dropout= 0.2,input_shape=(maxlen, HIDDNE_SIZE_1))(rnn1)\n",
    "    \n",
    "    z = Dropout(0.2)(rnn2)\n",
    "    \n",
    "    fully1 = Dense(HIDDNE_SIZE_2, activation='relu')(z)\n",
    "    \n",
    "    outp = Dense(6, activation='sigmoid')(fully1)  \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file=\"./img/text_rnn_model1.png\",show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/text_rnn_model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "9000/9000 [==============================] - 79s 9ms/step - loss: 0.2233 - acc: 0.9141 - val_loss: 0.1333 - val_acc: 0.9662\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.751257 \n",
      "\n",
      "Epoch 2/3\n",
      "9000/9000 [==============================] - 76s 8ms/step - loss: 0.1297 - acc: 0.9639 - val_loss: 0.0900 - val_acc: 0.9730\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.920605 \n",
      "\n",
      "Epoch 3/3\n",
      "9000/9000 [==============================] - 79s 9ms/step - loss: 0.0980 - acc: 0.9700 - val_loss: 0.0848 - val_acc: 0.9740\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.925773 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN+LSTM 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 100000\n",
    "# maxlen = 200\n",
    "# embed_size = 300\n",
    "\n",
    "HIDDNE_SIZE_1 = 256\n",
    "HIDDNE_SIZE_2 = 128\n",
    "\n",
    "def get_model_1():\n",
    "    ###embedding 和textcnn一样处理\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    #conv_0 = Conv1D(num_filters, kernel_size=kernel_size, strides=1)(x_emb)#这里也可以用conv1D，因为在embed_size\n",
    "    #等于词向量维度大小，故在列方向相当于没有做卷积操作，使用Conv2D的效果和Conv1D一样\n",
    "    lstm1 = LSTM(HIDDNE_SIZE_1, return_sequences=True, recurrent_dropout= 0.2,input_shape=(maxlen, embed_size))(x)\n",
    "    \n",
    "    lstm2 = LSTM(HIDDNE_SIZE_1, return_sequences=False, recurrent_dropout= 0.2,\n",
    "                      input_shape=(maxlen, HIDDNE_SIZE_1))(lstm1)\n",
    "    \n",
    "    z = Dropout(0.2)(lstm2)\n",
    "    \n",
    "    fully1 = Dense(HIDDNE_SIZE_2, activation='relu')(z)\n",
    "    \n",
    "    outp = Dense(6, activation='sigmoid')(fully1)  \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "9000/9000 [==============================] - 288s 32ms/step - loss: 0.2057 - acc: 0.9440 - val_loss: 0.1328 - val_acc: 0.9662\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.924230 \n",
      "\n",
      "Epoch 2/3\n",
      "9000/9000 [==============================] - 261s 29ms/step - loss: 0.1331 - acc: 0.9628 - val_loss: 0.1050 - val_acc: 0.9662\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.939407 \n",
      "\n",
      "Epoch 3/3\n",
      "9000/9000 [==============================] - 272s 30ms/step - loss: 0.0818 - acc: 0.9724 - val_loss: 0.0648 - val_acc: 0.9757\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.962307 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model_1()\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "RocAuc_1 = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"./img/text_rnn_lstm_model.png\",show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/text_rnn_lstm_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN+GRU 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Design Software\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "9000/9000 [==============================] - 86s 10ms/step - loss: 0.2827 - acc: 0.9227 - val_loss: 0.1226 - val_acc: 0.9662\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.752879 \n",
      "\n",
      "Epoch 2/3\n",
      "9000/9000 [==============================] - 83s 9ms/step - loss: 0.1229 - acc: 0.9632 - val_loss: 0.1006 - val_acc: 0.9683\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.849458 \n",
      "\n",
      "Epoch 3/3\n",
      "9000/9000 [==============================] - 82s 9ms/step - loss: 0.0888 - acc: 0.9701 - val_loss: 0.0759 - val_acc: 0.9777\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.942275 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"./img/text_rnn_gru1_model.png\",show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/text_rnn_gru1_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDNE_SIZE_1 = 256\n",
    "HIDDNE_SIZE_2 = 128\n",
    "\n",
    "def get_model():\n",
    "    ###embedding 和textcnn一样处理\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    #conv_0 = Conv1D(num_filters, kernel_size=kernel_size, strides=1)(x_emb)#这里也可以用conv1D，因为在embed_size\n",
    "    #等于词向量维度大小，故在列方向相当于没有做卷积操作，使用Conv2D的效果和Conv1D一样\n",
    "    lstm1 = GRU(HIDDNE_SIZE_1, return_sequences=True, recurrent_dropout= 0.2,input_shape=(maxlen, embed_size))(x)\n",
    "    \n",
    "    lstm2 = GRU(HIDDNE_SIZE_1, return_sequences=False, recurrent_dropout= 0.2,\n",
    "                      input_shape=(maxlen, HIDDNE_SIZE_1))(lstm1)\n",
    "    \n",
    "    z = Dropout(0.2)(lstm2)\n",
    "    \n",
    "    fully1 = Dense(HIDDNE_SIZE_2, activation='relu')(z)\n",
    "    \n",
    "    outp = Dense(6, activation='sigmoid')(fully1)  \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "9000/9000 [==============================] - 211s 23ms/step - loss: 0.1828 - acc: 0.9556 - val_loss: 0.0791 - val_acc: 0.9757\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.933086 \n",
      "\n",
      "Epoch 2/3\n",
      "9000/9000 [==============================] - 205s 23ms/step - loss: 0.0747 - acc: 0.9755 - val_loss: 0.0621 - val_acc: 0.9787\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.967686 \n",
      "\n",
      "Epoch 3/3\n",
      "9000/9000 [==============================] - 206s 23ms/step - loss: 0.0541 - acc: 0.9808 - val_loss: 0.0537 - val_acc: 0.9800\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.975563 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"./img/text_rnn_gru2_model.png\",show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/text_rnn_gru2_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
